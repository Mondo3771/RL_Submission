Started at: Wed Oct 29 03:30:04 AM SAST 2025
LETS COOOK IT UPPP
==============================================
        PPO + ICM Crafter Training Run        
==============================================
Using 8 parallel environments.
ICM enabled: True

Starting new training from scratch.

Using cuda device
Wrapping the env in a VecTransposeImage.

Training for 2,000,000 more timesteps...

Logging to ./PPO_crafter_tensorboard/PPO_2
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 166      |
|    ep_rew_mean     | 1.53     |
| time/              |          |
|    fps             | 133      |
|    iterations      | 1        |
|    time_elapsed    | 61       |
|    total_timesteps | 8192     |
---------------------------------
ICM Feature size: 1024
-------------------------------------------
| rollout/                 |              |
|    ep_len_mean           | 174          |
|    ep_rew_mean           | 1.37         |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 2            |
|    time_elapsed          | 134          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.0027516445 |
|    clip_fraction         | 0.0888       |
|    clip_range            | 0.1          |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0524      |
|    icm_loss              | 0.00818      |
|    intrinsic_reward_mean | 0.0036294777 |
|    intrinsic_reward_std  | 0.03933273   |
|    learning_rate         | 0.000249     |
|    loss                  | 0.0115       |
|    n_updates             | 4            |
|    policy_gradient_loss  | -0.00443     |
|    value_loss            | 0.0943       |
-------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 175           |
|    ep_rew_mean           | 1.5           |
| time/                    |               |
|    fps                   | 116           |
|    iterations            | 3             |
|    time_elapsed          | 211           |
|    total_timesteps       | 24576         |
| train/                   |               |
|    approx_kl             | 0.0020195849  |
|    clip_fraction         | 0.0627        |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.83         |
|    explained_variance    | 0.431         |
|    icm_loss              | 1.05e-05      |
|    intrinsic_reward_mean | 1.0441634e-05 |
|    intrinsic_reward_std  | 1.1081661e-05 |
|    learning_rate         | 0.000248      |
|    loss                  | -0.00619      |
|    n_updates             | 8             |
|    policy_gradient_loss  | -0.00355      |
|    value_loss            | 0.064         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 174           |
|    ep_rew_mean           | 1.56          |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 4             |
|    time_elapsed          | 287           |
|    total_timesteps       | 32768         |
| train/                   |               |
|    approx_kl             | 0.0023008152  |
|    clip_fraction         | 0.121         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.82         |
|    explained_variance    | 0.483         |
|    icm_loss              | 3.76e-06      |
|    intrinsic_reward_mean | 3.7560612e-06 |
|    intrinsic_reward_std  | 3.7715586e-06 |
|    learning_rate         | 0.000247      |
|    loss                  | 0.0174        |
|    n_updates             | 12            |
|    policy_gradient_loss  | -0.0061       |
|    value_loss            | 0.114         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 174           |
|    ep_rew_mean           | 1.72          |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 5             |
|    time_elapsed          | 365           |
|    total_timesteps       | 40960         |
| train/                   |               |
|    approx_kl             | 0.0022425326  |
|    clip_fraction         | 0.104         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.82         |
|    explained_variance    | 0.632         |
|    icm_loss              | 1.57e-06      |
|    intrinsic_reward_mean | 1.5682494e-06 |
|    intrinsic_reward_std  | 2.170742e-06  |
|    learning_rate         | 0.000246      |
|    loss                  | 0.00309       |
|    n_updates             | 16            |
|    policy_gradient_loss  | -0.00515      |
|    value_loss            | 0.0977        |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 176           |
|    ep_rew_mean           | 1.74          |
| time/                    |               |
|    fps                   | 111           |
|    iterations            | 6             |
|    time_elapsed          | 440           |
|    total_timesteps       | 49152         |
| train/                   |               |
|    approx_kl             | 0.0024705091  |
|    clip_fraction         | 0.107         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.81         |
|    explained_variance    | 0.678         |
|    icm_loss              | 2.82e-07      |
|    intrinsic_reward_mean | 2.8244256e-07 |
|    intrinsic_reward_std  | 7.157446e-07  |
|    learning_rate         | 0.000245      |
|    loss                  | 0.000824      |
|    n_updates             | 20            |
|    policy_gradient_loss  | -0.0066       |
|    value_loss            | 0.103         |
--------------------------------------------
Eval num_timesteps=50000, episode_reward=1.30 +/- 0.60
Episode length: 175.60 +/- 33.54
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 176           |
|    mean_reward           | 1.3           |
| time/                    |               |
|    total_timesteps       | 50000         |
| train/                   |               |
|    approx_kl             | 0.0025526448  |
|    clip_fraction         | 0.109         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.81         |
|    explained_variance    | 0.736         |
|    icm_loss              | 1.35e-07      |
|    intrinsic_reward_mean | 1.3473772e-07 |
|    intrinsic_reward_std  | 2.872054e-07  |
|    learning_rate         | 0.000244      |
|    loss                  | 0.00186       |
|    n_updates             | 24            |
|    policy_gradient_loss  | -0.00596      |
|    value_loss            | 0.0903        |
--------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 173      |
|    ep_rew_mean     | 1.9      |
| time/              |          |
|    fps             | 106      |
|    iterations      | 7        |
|    time_elapsed    | 536      |
|    total_timesteps | 57344    |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 166           |
|    ep_rew_mean           | 1.99          |
| time/                    |               |
|    fps                   | 107           |
|    iterations            | 8             |
|    time_elapsed          | 612           |
|    total_timesteps       | 65536         |
| train/                   |               |
|    approx_kl             | 0.0024205418  |
|    clip_fraction         | 0.139         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.8          |
|    explained_variance    | 0.718         |
|    icm_loss              | 8.53e-08      |
|    intrinsic_reward_mean | 8.525676e-08  |
|    intrinsic_reward_std  | 1.9423288e-07 |
|    learning_rate         | 0.000243      |
|    loss                  | 0.00488       |
|    n_updates             | 28            |
|    policy_gradient_loss  | -0.00738      |
|    value_loss            | 0.12          |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 168           |
|    ep_rew_mean           | 2.15          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 9             |
|    time_elapsed          | 690           |
|    total_timesteps       | 73728         |
| train/                   |               |
|    approx_kl             | 0.0025243065  |
|    clip_fraction         | 0.121         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.79         |
|    explained_variance    | 0.733         |
|    icm_loss              | 5.87e-08      |
|    intrinsic_reward_mean | 5.8713653e-08 |
|    intrinsic_reward_std  | 1.9766698e-07 |
|    learning_rate         | 0.000242      |
|    loss                  | 0.000432      |
|    n_updates             | 32            |
|    policy_gradient_loss  | -0.00622      |
|    value_loss            | 0.114         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 171           |
|    ep_rew_mean           | 2.24          |
| time/                    |               |
|    fps                   | 107           |
|    iterations            | 10            |
|    time_elapsed          | 765           |
|    total_timesteps       | 81920         |
| train/                   |               |
|    approx_kl             | 0.0024377978  |
|    clip_fraction         | 0.134         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.78         |
|    explained_variance    | 0.742         |
|    icm_loss              | 4.23e-08      |
|    intrinsic_reward_mean | 4.2298318e-08 |
|    intrinsic_reward_std  | 2.1463227e-07 |
|    learning_rate         | 0.000241      |
|    loss                  | 0.00983       |
|    n_updates             | 36            |
|    policy_gradient_loss  | -0.00641      |
|    value_loss            | 0.123         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 164           |
|    ep_rew_mean           | 2.13          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 11            |
|    time_elapsed          | 844           |
|    total_timesteps       | 90112         |
| train/                   |               |
|    approx_kl             | 0.0030079167  |
|    clip_fraction         | 0.147         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.78         |
|    explained_variance    | 0.751         |
|    icm_loss              | 9e-08         |
|    intrinsic_reward_mean | 9.001079e-08  |
|    intrinsic_reward_std  | 1.3896506e-06 |
|    learning_rate         | 0.00024       |
|    loss                  | -0.00374      |
|    n_updates             | 40            |
|    policy_gradient_loss  | -0.0062       |
|    value_loss            | 0.114         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 168           |
|    ep_rew_mean           | 2.14          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 12            |
|    time_elapsed          | 919           |
|    total_timesteps       | 98304         |
| train/                   |               |
|    approx_kl             | 0.0031632422  |
|    clip_fraction         | 0.164         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.76         |
|    explained_variance    | 0.762         |
|    icm_loss              | 2.15e-08      |
|    intrinsic_reward_mean | 2.1469905e-08 |
|    intrinsic_reward_std  | 1.2927315e-07 |
|    learning_rate         | 0.000239      |
|    loss                  | 0.0173        |
|    n_updates             | 44            |
|    policy_gradient_loss  | -0.00797      |
|    value_loss            | 0.116         |
--------------------------------------------
Eval num_timesteps=100000, episode_reward=1.60 +/- 0.92
Episode length: 182.10 +/- 54.60
-------------------------------------------
| eval/                    |              |
|    mean_ep_length        | 182          |
|    mean_reward           | 1.6          |
| time/                    |              |
|    total_timesteps       | 100000       |
| train/                   |              |
|    approx_kl             | 0.002885954  |
|    clip_fraction         | 0.146        |
|    clip_range            | 0.1          |
|    entropy_loss          | -2.75        |
|    explained_variance    | 0.756        |
|    icm_loss              | 8.62e-09     |
|    intrinsic_reward_mean | 8.619453e-09 |
|    intrinsic_reward_std  | 3.551197e-08 |
|    learning_rate         | 0.000238     |
|    loss                  | 0.000538     |
|    n_updates             | 48           |
|    policy_gradient_loss  | -0.00721     |
|    value_loss            | 0.106        |
-------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 2.3      |
| time/              |          |
|    fps             | 105      |
|    iterations      | 13       |
|    time_elapsed    | 1009     |
|    total_timesteps | 106496   |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 178           |
|    ep_rew_mean           | 2.52          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 14            |
|    time_elapsed          | 1086          |
|    total_timesteps       | 114688        |
| train/                   |               |
|    approx_kl             | 0.0032797717  |
|    clip_fraction         | 0.158         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.75         |
|    explained_variance    | 0.746         |
|    icm_loss              | 5.91e-09      |
|    intrinsic_reward_mean | 5.9071095e-09 |
|    intrinsic_reward_std  | 2.8403452e-08 |
|    learning_rate         | 0.000237      |
|    loss                  | 0.00217       |
|    n_updates             | 52            |
|    policy_gradient_loss  | -0.00717      |
|    value_loss            | 0.0989        |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 172           |
|    ep_rew_mean           | 2.57          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 15            |
|    time_elapsed          | 1159          |
|    total_timesteps       | 122880        |
| train/                   |               |
|    approx_kl             | 0.0031021181  |
|    clip_fraction         | 0.16          |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.74         |
|    explained_variance    | 0.709         |
|    icm_loss              | 5.28e-09      |
|    intrinsic_reward_mean | 5.2838756e-09 |
|    intrinsic_reward_std  | 3.149042e-08  |
|    learning_rate         | 0.000236      |
|    loss                  | 0.00979       |
|    n_updates             | 56            |
|    policy_gradient_loss  | -0.00796      |
|    value_loss            | 0.114         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 175           |
|    ep_rew_mean           | 2.74          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 16            |
|    time_elapsed          | 1239          |
|    total_timesteps       | 131072        |
| train/                   |               |
|    approx_kl             | 0.0036769486  |
|    clip_fraction         | 0.191         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.74         |
|    explained_variance    | 0.717         |
|    icm_loss              | 2.93e-09      |
|    intrinsic_reward_mean | 2.9263534e-09 |
|    intrinsic_reward_std  | 7.881915e-09  |
|    learning_rate         | 0.000235      |
|    loss                  | 0.00374       |
|    n_updates             | 60            |
|    policy_gradient_loss  | -0.00844      |
|    value_loss            | 0.111         |
--------------------------------------------
-------------------------------------------
| rollout/                 |              |
|    ep_len_mean           | 168          |
|    ep_rew_mean           | 2.73         |
| time/                    |              |
|    fps                   | 105          |
|    iterations            | 17           |
|    time_elapsed          | 1318         |
|    total_timesteps       | 139264       |
| train/                   |              |
|    approx_kl             | 0.0035202182 |
|    clip_fraction         | 0.173        |
|    clip_range            | 0.1          |
|    entropy_loss          | -2.71        |
|    explained_variance    | 0.734        |
|    icm_loss              | 2.05e-09     |
|    intrinsic_reward_mean | 2.051111e-09 |
|    intrinsic_reward_std  | 5.134221e-09 |
|    learning_rate         | 0.000234     |
|    loss                  | -0.00111     |
|    n_updates             | 64           |
|    policy_gradient_loss  | -0.00893     |
|    value_loss            | 0.109        |
-------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 168           |
|    ep_rew_mean           | 2.72          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 18            |
|    time_elapsed          | 1395          |
|    total_timesteps       | 147456        |
| train/                   |               |
|    approx_kl             | 0.0035158058  |
|    clip_fraction         | 0.192         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.7          |
|    explained_variance    | 0.716         |
|    icm_loss              | 2.45e-09      |
|    intrinsic_reward_mean | 2.4516913e-09 |
|    intrinsic_reward_std  | 4.0637644e-09 |
|    learning_rate         | 0.000233      |
|    loss                  | 0.0115        |
|    n_updates             | 68            |
|    policy_gradient_loss  | -0.00845      |
|    value_loss            | 0.118         |
--------------------------------------------
Eval num_timesteps=150000, episode_reward=2.40 +/- 1.10
Episode length: 199.70 +/- 33.38
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 200           |
|    mean_reward           | 2.4           |
| time/                    |               |
|    total_timesteps       | 150000        |
| train/                   |               |
|    approx_kl             | 0.0033527832  |
|    clip_fraction         | 0.174         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.7          |
|    explained_variance    | 0.762         |
|    icm_loss              | 1.96e-09      |
|    intrinsic_reward_mean | 1.9617916e-09 |
|    intrinsic_reward_std  | 4.032735e-09  |
|    learning_rate         | 0.000232      |
|    loss                  | -0.0143       |
|    n_updates             | 72            |
|    policy_gradient_loss  | -0.00929      |
|    value_loss            | 0.101         |
--------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 2.92     |
| time/              |          |
|    fps             | 104      |
|    iterations      | 19       |
|    time_elapsed    | 1483     |
|    total_timesteps | 155648   |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 179           |
|    ep_rew_mean           | 3             |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 20            |
|    time_elapsed          | 1562          |
|    total_timesteps       | 163840        |
| train/                   |               |
|    approx_kl             | 0.003647313   |
|    clip_fraction         | 0.211         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.68         |
|    explained_variance    | 0.783         |
|    icm_loss              | 1.99e-09      |
|    intrinsic_reward_mean | 1.9900375e-09 |
|    intrinsic_reward_std  | 3.2738994e-09 |
|    learning_rate         | 0.000231      |
|    loss                  | -0.011        |
|    n_updates             | 76            |
|    policy_gradient_loss  | -0.0108       |
|    value_loss            | 0.0994        |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 172           |
|    ep_rew_mean           | 2.97          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 21            |
|    time_elapsed          | 1637          |
|    total_timesteps       | 172032        |
| train/                   |               |
|    approx_kl             | 0.0038491955  |
|    clip_fraction         | 0.182         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.69         |
|    explained_variance    | 0.779         |
|    icm_loss              | 3.22e-09      |
|    intrinsic_reward_mean | 3.219213e-09  |
|    intrinsic_reward_std  | 1.7650265e-08 |
|    learning_rate         | 0.00023       |
|    loss                  | -0.00558      |
|    n_updates             | 80            |
|    policy_gradient_loss  | -0.00873      |
|    value_loss            | 0.104         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 170           |
|    ep_rew_mean           | 3.05          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 22            |
|    time_elapsed          | 1711          |
|    total_timesteps       | 180224        |
| train/                   |               |
|    approx_kl             | 0.003916996   |
|    clip_fraction         | 0.199         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.68         |
|    explained_variance    | 0.749         |
|    icm_loss              | 1.73e-09      |
|    intrinsic_reward_mean | 1.7288129e-09 |
|    intrinsic_reward_std  | 3.1079648e-09 |
|    learning_rate         | 0.000228      |
|    loss                  | -0.00991      |
|    n_updates             | 84            |
|    policy_gradient_loss  | -0.00945      |
|    value_loss            | 0.119         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 180           |
|    ep_rew_mean           | 3.13          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 23            |
|    time_elapsed          | 1786          |
|    total_timesteps       | 188416        |
| train/                   |               |
|    approx_kl             | 0.0035660227  |
|    clip_fraction         | 0.193         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.67         |
|    explained_variance    | 0.782         |
|    icm_loss              | 1.8e-09       |
|    intrinsic_reward_mean | 1.8024795e-09 |
|    intrinsic_reward_std  | 3.4085776e-09 |
|    learning_rate         | 0.000227      |
|    loss                  | 0.0045        |
|    n_updates             | 88            |
|    policy_gradient_loss  | -0.00951      |
|    value_loss            | 0.113         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 175           |
|    ep_rew_mean           | 3.08          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 24            |
|    time_elapsed          | 1865          |
|    total_timesteps       | 196608        |
| train/                   |               |
|    approx_kl             | 0.003647497   |
|    clip_fraction         | 0.205         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.66         |
|    explained_variance    | 0.777         |
|    icm_loss              | 1.78e-09      |
|    intrinsic_reward_mean | 1.7810462e-09 |
|    intrinsic_reward_std  | 2.619411e-09  |
|    learning_rate         | 0.000226      |
|    loss                  | -0.00297      |
|    n_updates             | 92            |
|    policy_gradient_loss  | -0.00924      |
|    value_loss            | 0.114         |
--------------------------------------------
Eval num_timesteps=200000, episode_reward=2.40 +/- 0.64
Episode length: 152.60 +/- 32.15
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 153           |
|    mean_reward           | 2.4           |
| time/                    |               |
|    total_timesteps       | 200000        |
| train/                   |               |
|    approx_kl             | 0.0039245714  |
|    clip_fraction         | 0.216         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.63         |
|    explained_variance    | 0.791         |
|    icm_loss              | 1.59e-09      |
|    intrinsic_reward_mean | 1.5900213e-09 |
|    intrinsic_reward_std  | 2.5991564e-09 |
|    learning_rate         | 0.000225      |
|    loss                  | 0.0106        |
|    n_updates             | 96            |
|    policy_gradient_loss  | -0.0092       |
|    value_loss            | 0.119         |
--------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | 3.21     |
| time/              |          |
|    fps             | 104      |
|    iterations      | 25       |
|    time_elapsed    | 1957     |
|    total_timesteps | 204800   |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 172           |
|    ep_rew_mean           | 3.22          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 26            |
|    time_elapsed          | 2035          |
|    total_timesteps       | 212992        |
| train/                   |               |
|    approx_kl             | 0.0041528232  |
|    clip_fraction         | 0.21          |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.62         |
|    explained_variance    | 0.785         |
|    icm_loss              | 1.57e-09      |
|    intrinsic_reward_mean | 1.5654029e-09 |
|    intrinsic_reward_std  | 2.056195e-09  |
|    learning_rate         | 0.000224      |
|    loss                  | -0.013        |
|    n_updates             | 100           |
|    policy_gradient_loss  | -0.0107       |
|    value_loss            | 0.108         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 174           |
|    ep_rew_mean           | 3.31          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 27            |
|    time_elapsed          | 2110          |
|    total_timesteps       | 221184        |
| train/                   |               |
|    approx_kl             | 0.0044711153  |
|    clip_fraction         | 0.225         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.62         |
|    explained_variance    | 0.761         |
|    icm_loss              | 1.48e-09      |
|    intrinsic_reward_mean | 1.4767282e-09 |
|    intrinsic_reward_std  | 1.8056169e-09 |
|    learning_rate         | 0.000223      |
|    loss                  | 0.0023        |
|    n_updates             | 104           |
|    policy_gradient_loss  | -0.0116       |
|    value_loss            | 0.117         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 179           |
|    ep_rew_mean           | 3.59          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 28            |
|    time_elapsed          | 2185          |
|    total_timesteps       | 229376        |
| train/                   |               |
|    approx_kl             | 0.004378316   |
|    clip_fraction         | 0.217         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.61         |
|    explained_variance    | 0.792         |
|    icm_loss              | 2.26e-09      |
|    intrinsic_reward_mean | 2.2612525e-09 |
|    intrinsic_reward_std  | 7.934592e-09  |
|    learning_rate         | 0.000222      |
|    loss                  | -0.000598     |
|    n_updates             | 108           |
|    policy_gradient_loss  | -0.00976      |
|    value_loss            | 0.108         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 182           |
|    ep_rew_mean           | 3.55          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 29            |
|    time_elapsed          | 2262          |
|    total_timesteps       | 237568        |
| train/                   |               |
|    approx_kl             | 0.0046369387  |
|    clip_fraction         | 0.23          |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.59         |
|    explained_variance    | 0.798         |
|    icm_loss              | 1.79e-09      |
|    intrinsic_reward_mean | 1.7947075e-09 |
|    intrinsic_reward_std  | 2.2813044e-08 |
|    learning_rate         | 0.000221      |
|    loss                  | -0.00951      |
|    n_updates             | 112           |
|    policy_gradient_loss  | -0.0106       |
|    value_loss            | 0.101         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 176           |
|    ep_rew_mean           | 3.35          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 30            |
|    time_elapsed          | 2341          |
|    total_timesteps       | 245760        |
| train/                   |               |
|    approx_kl             | 0.0042616352  |
|    clip_fraction         | 0.224         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.58         |
|    explained_variance    | 0.821         |
|    icm_loss              | 1.28e-09      |
|    intrinsic_reward_mean | 1.2817456e-09 |
|    intrinsic_reward_std  | 1.3827596e-09 |
|    learning_rate         | 0.00022       |
|    loss                  | -0.00305      |
|    n_updates             | 116           |
|    policy_gradient_loss  | -0.00987      |
|    value_loss            | 0.112         |
--------------------------------------------
Eval num_timesteps=250000, episode_reward=2.50 +/- 0.49
Episode length: 213.20 +/- 79.09
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 213           |
|    mean_reward           | 2.5           |
| time/                    |               |
|    total_timesteps       | 250000        |
| train/                   |               |
|    approx_kl             | 0.0048953835  |
|    clip_fraction         | 0.242         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.58         |
|    explained_variance    | 0.802         |
|    icm_loss              | 1.21e-09      |
|    intrinsic_reward_mean | 1.2102874e-09 |
|    intrinsic_reward_std  | 1.27747e-09   |
|    learning_rate         | 0.000219      |
|    loss                  | -0.0158       |
|    n_updates             | 120           |
|    policy_gradient_loss  | -0.0108       |
|    value_loss            | 0.103         |
--------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 3.54     |
| time/              |          |
|    fps             | 104      |
|    iterations      | 31       |
|    time_elapsed    | 2430     |
|    total_timesteps | 253952   |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 176           |
|    ep_rew_mean           | 3.62          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 32            |
|    time_elapsed          | 2505          |
|    total_timesteps       | 262144        |
| train/                   |               |
|    approx_kl             | 0.0045310715  |
|    clip_fraction         | 0.219         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.58         |
|    explained_variance    | 0.804         |
|    icm_loss              | 1.22e-09      |
|    intrinsic_reward_mean | 1.2228054e-09 |
|    intrinsic_reward_std  | 1.1419087e-09 |
|    learning_rate         | 0.000218      |
|    loss                  | -0.00777      |
|    n_updates             | 124           |
|    policy_gradient_loss  | -0.00947      |
|    value_loss            | 0.108         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 179           |
|    ep_rew_mean           | 3.57          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 33            |
|    time_elapsed          | 2582          |
|    total_timesteps       | 270336        |
| train/                   |               |
|    approx_kl             | 0.0041280324  |
|    clip_fraction         | 0.226         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.57         |
|    explained_variance    | 0.818         |
|    icm_loss              | 1.14e-09      |
|    intrinsic_reward_mean | 1.1422516e-09 |
|    intrinsic_reward_std  | 1.1365343e-09 |
|    learning_rate         | 0.000217      |
|    loss                  | 0.0134        |
|    n_updates             | 128           |
|    policy_gradient_loss  | -0.0112       |
|    value_loss            | 0.118         |
--------------------------------------------
-------------------------------------------
| rollout/                 |              |
|    ep_len_mean           | 174          |
|    ep_rew_mean           | 3.75         |
| time/                    |              |
|    fps                   | 104          |
|    iterations            | 34           |
|    time_elapsed          | 2660         |
|    total_timesteps       | 278528       |
| train/                   |              |
|    approx_kl             | 0.0041330224 |
|    clip_fraction         | 0.218        |
|    clip_range            | 0.1          |
|    entropy_loss          | -2.56        |
|    explained_variance    | 0.814        |
|    icm_loss              | 1.13e-09     |
|    intrinsic_reward_mean | 1.134713e-09 |
|    intrinsic_reward_std  | 9.713026e-10 |
|    learning_rate         | 0.000216     |
|    loss                  | -0.00134     |
|    n_updates             | 132          |
|    policy_gradient_loss  | -0.00968     |
|    value_loss            | 0.124        |
-------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 170           |
|    ep_rew_mean           | 3.82          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 35            |
|    time_elapsed          | 2736          |
|    total_timesteps       | 286720        |
| train/                   |               |
|    approx_kl             | 0.0043743374  |
|    clip_fraction         | 0.236         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.54         |
|    explained_variance    | 0.82          |
|    icm_loss              | 1.11e-09      |
|    intrinsic_reward_mean | 1.1050848e-09 |
|    intrinsic_reward_std  | 1.1676532e-09 |
|    learning_rate         | 0.000215      |
|    loss                  | 0.00125       |
|    n_updates             | 136           |
|    policy_gradient_loss  | -0.0108       |
|    value_loss            | 0.113         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 183           |
|    ep_rew_mean           | 3.98          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 36            |
|    time_elapsed          | 2807          |
|    total_timesteps       | 294912        |
| train/                   |               |
|    approx_kl             | 0.00480863    |
|    clip_fraction         | 0.25          |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.53         |
|    explained_variance    | 0.833         |
|    icm_loss              | 1.06e-09      |
|    intrinsic_reward_mean | 1.0620189e-09 |
|    intrinsic_reward_std  | 9.519795e-10  |
|    learning_rate         | 0.000214      |
|    loss                  | -0.00595      |
|    n_updates             | 140           |
|    policy_gradient_loss  | -0.00941      |
|    value_loss            | 0.119         |
--------------------------------------------
Eval num_timesteps=300000, episode_reward=3.30 +/- 0.87
Episode length: 182.20 +/- 24.66
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 182           |
|    mean_reward           | 3.3           |
| time/                    |               |
|    total_timesteps       | 300000        |
| train/                   |               |
|    approx_kl             | 0.004787937   |
|    clip_fraction         | 0.236         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.55         |
|    explained_variance    | 0.844         |
|    icm_loss              | 1.03e-09      |
|    intrinsic_reward_mean | 1.0291358e-09 |
|    intrinsic_reward_std  | 8.6882773e-10 |
|    learning_rate         | 0.000213      |
|    loss                  | -0.0129       |
|    n_updates             | 144           |
|    policy_gradient_loss  | -0.011        |
|    value_loss            | 0.104         |
--------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 4.06     |
| time/              |          |
|    fps             | 104      |
|    iterations      | 37       |
|    time_elapsed    | 2899     |
|    total_timesteps | 303104   |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 181           |
|    ep_rew_mean           | 3.94          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 38            |
|    time_elapsed          | 2976          |
|    total_timesteps       | 311296        |
| train/                   |               |
|    approx_kl             | 0.0045248372  |
|    clip_fraction         | 0.239         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.55         |
|    explained_variance    | 0.85          |
|    icm_loss              | 1e-09         |
|    intrinsic_reward_mean | 1.0023082e-09 |
|    intrinsic_reward_std  | 8.367956e-10  |
|    learning_rate         | 0.000212      |
|    loss                  | -0.00275      |
|    n_updates             | 148           |
|    policy_gradient_loss  | -0.0101       |
|    value_loss            | 0.116         |
--------------------------------------------
-------------------------------------------
| rollout/                 |              |
|    ep_len_mean           | 177          |
|    ep_rew_mean           | 3.83         |
| time/                    |              |
|    fps                   | 104          |
|    iterations            | 39           |
|    time_elapsed          | 3053         |
|    total_timesteps       | 319488       |
| train/                   |              |
|    approx_kl             | 0.0044208574 |
|    clip_fraction         | 0.23         |
|    clip_range            | 0.1          |
|    entropy_loss          | -2.54        |
|    explained_variance    | 0.853        |
|    icm_loss              | 9.9e-10      |
|    intrinsic_reward_mean | 9.904966e-10 |
|    intrinsic_reward_std  | 7.141325e-10 |
|    learning_rate         | 0.000211     |
|    loss                  | -0.000143    |
|    n_updates             | 152          |
|    policy_gradient_loss  | -0.00985     |
|    value_loss            | 0.107        |
-------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 172           |
|    ep_rew_mean           | 3.77          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 40            |
|    time_elapsed          | 3129          |
|    total_timesteps       | 327680        |
| train/                   |               |
|    approx_kl             | 0.0046009645  |
|    clip_fraction         | 0.23          |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.53         |
|    explained_variance    | 0.85          |
|    icm_loss              | 9.36e-10      |
|    intrinsic_reward_mean | 9.362328e-10  |
|    intrinsic_reward_std  | 7.3050826e-10 |
|    learning_rate         | 0.00021       |
|    loss                  | -0.00263      |
|    n_updates             | 156           |
|    policy_gradient_loss  | -0.0095       |
|    value_loss            | 0.115         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 173           |
|    ep_rew_mean           | 3.77          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 41            |
|    time_elapsed          | 3207          |
|    total_timesteps       | 335872        |
| train/                   |               |
|    approx_kl             | 0.0054523237  |
|    clip_fraction         | 0.26          |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.55         |
|    explained_variance    | 0.837         |
|    icm_loss              | 9.39e-10      |
|    intrinsic_reward_mean | 9.389267e-10  |
|    intrinsic_reward_std  | 6.7010647e-10 |
|    learning_rate         | 0.000209      |
|    loss                  | -0.00316      |
|    n_updates             | 160           |
|    policy_gradient_loss  | -0.0109       |
|    value_loss            | 0.121         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 170           |
|    ep_rew_mean           | 3.82          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 42            |
|    time_elapsed          | 3286          |
|    total_timesteps       | 344064        |
| train/                   |               |
|    approx_kl             | 0.0045680557  |
|    clip_fraction         | 0.25          |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.55         |
|    explained_variance    | 0.846         |
|    icm_loss              | 9.14e-10      |
|    intrinsic_reward_mean | 9.1372665e-10 |
|    intrinsic_reward_std  | 7.2050266e-10 |
|    learning_rate         | 0.000208      |
|    loss                  | -0.016        |
|    n_updates             | 164           |
|    policy_gradient_loss  | -0.0103       |
|    value_loss            | 0.109         |
--------------------------------------------
Eval num_timesteps=350000, episode_reward=2.70 +/- 0.66
Episode length: 201.70 +/- 31.21
-------------------------------------------
| eval/                    |              |
|    mean_ep_length        | 202          |
|    mean_reward           | 2.7          |
| time/                    |              |
|    total_timesteps       | 350000       |
| train/                   |              |
|    approx_kl             | 0.0054508257 |
|    clip_fraction         | 0.253        |
|    clip_range            | 0.1          |
|    entropy_loss          | -2.51        |
|    explained_variance    | 0.812        |
|    icm_loss              | 8.91e-10     |
|    intrinsic_reward_mean | 8.909804e-10 |
|    intrinsic_reward_std  | 7.218426e-10 |
|    learning_rate         | 0.000207     |
|    loss                  | 0.00668      |
|    n_updates             | 168          |
|    policy_gradient_loss  | -0.0109      |
|    value_loss            | 0.13         |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 3.98     |
| time/              |          |
|    fps             | 104      |
|    iterations      | 43       |
|    time_elapsed    | 3375     |
|    total_timesteps | 352256   |
---------------------------------
-------------------------------------------
| rollout/                 |              |
|    ep_len_mean           | 186          |
|    ep_rew_mean           | 4.05         |
| time/                    |              |
|    fps                   | 104          |
|    iterations            | 44           |
|    time_elapsed          | 3449         |
|    total_timesteps       | 360448       |
| train/                   |              |
|    approx_kl             | 0.005325528  |
|    clip_fraction         | 0.244        |
|    clip_range            | 0.1          |
|    entropy_loss          | -2.5         |
|    explained_variance    | 0.857        |
|    icm_loss              | 8.69e-10     |
|    intrinsic_reward_mean | 8.689632e-10 |
|    intrinsic_reward_std  | 6.601132e-10 |
|    learning_rate         | 0.000206     |
|    loss                  | -0.0113      |
|    n_updates             | 172          |
|    policy_gradient_loss  | -0.0106      |
|    value_loss            | 0.104        |
-------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 180           |
|    ep_rew_mean           | 3.85          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 45            |
|    time_elapsed          | 3527          |
|    total_timesteps       | 368640        |
| train/                   |               |
|    approx_kl             | 0.005220686   |
|    clip_fraction         | 0.262         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.5          |
|    explained_variance    | 0.867         |
|    icm_loss              | 8.46e-10      |
|    intrinsic_reward_mean | 8.459533e-10  |
|    intrinsic_reward_std  | 5.1654375e-10 |
|    learning_rate         | 0.000205      |
|    loss                  | -0.00391      |
|    n_updates             | 176           |
|    policy_gradient_loss  | -0.0104       |
|    value_loss            | 0.104         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 181           |
|    ep_rew_mean           | 3.91          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 46            |
|    time_elapsed          | 3598          |
|    total_timesteps       | 376832        |
| train/                   |               |
|    approx_kl             | 0.005690798   |
|    clip_fraction         | 0.267         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.5          |
|    explained_variance    | 0.846         |
|    icm_loss              | 8.4e-10       |
|    intrinsic_reward_mean | 8.4011104e-10 |
|    intrinsic_reward_std  | 5.251222e-10  |
|    learning_rate         | 0.000204      |
|    loss                  | -0.0122       |
|    n_updates             | 180           |
|    policy_gradient_loss  | -0.00987      |
|    value_loss            | 0.113         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 182           |
|    ep_rew_mean           | 4.15          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 47            |
|    time_elapsed          | 3674          |
|    total_timesteps       | 385024        |
| train/                   |               |
|    approx_kl             | 0.0055227876  |
|    clip_fraction         | 0.283         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.51         |
|    explained_variance    | 0.859         |
|    icm_loss              | 8.15e-10      |
|    intrinsic_reward_mean | 8.1488427e-10 |
|    intrinsic_reward_std  | 4.7459175e-10 |
|    learning_rate         | 0.000203      |
|    loss                  | -0.00818      |
|    n_updates             | 184           |
|    policy_gradient_loss  | -0.0109       |
|    value_loss            | 0.0985        |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 175           |
|    ep_rew_mean           | 4.07          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 48            |
|    time_elapsed          | 3755          |
|    total_timesteps       | 393216        |
| train/                   |               |
|    approx_kl             | 0.0057614543  |
|    clip_fraction         | 0.276         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.5          |
|    explained_variance    | 0.825         |
|    icm_loss              | 8.09e-10      |
|    intrinsic_reward_mean | 8.0897733e-10 |
|    intrinsic_reward_std  | 5.727666e-10  |
|    learning_rate         | 0.000202      |
|    loss                  | 0.00893       |
|    n_updates             | 188           |
|    policy_gradient_loss  | -0.0107       |
|    value_loss            | 0.12          |
--------------------------------------------
Eval num_timesteps=400000, episode_reward=2.60 +/- 0.81
Episode length: 174.40 +/- 43.73
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 174           |
|    mean_reward           | 2.6           |
| time/                    |               |
|    total_timesteps       | 400000        |
| train/                   |               |
|    approx_kl             | 0.0057768323  |
|    clip_fraction         | 0.276         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.46         |
|    explained_variance    | 0.835         |
|    icm_loss              | 8.17e-10      |
|    intrinsic_reward_mean | 8.174805e-10  |
|    intrinsic_reward_std  | 7.3932743e-10 |
|    learning_rate         | 0.000201      |
|    loss                  | -0.000175     |
|    n_updates             | 192           |
|    policy_gradient_loss  | -0.00991      |
|    value_loss            | 0.112         |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 170      |
|    ep_rew_mean     | 4.04     |
| time/              |          |
|    fps             | 104      |
|    iterations      | 49       |
|    time_elapsed    | 3846     |
|    total_timesteps | 401408   |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 172           |
|    ep_rew_mean           | 4.06          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 50            |
|    time_elapsed          | 3924          |
|    total_timesteps       | 409600        |
| train/                   |               |
|    approx_kl             | 0.00561812    |
|    clip_fraction         | 0.266         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.42         |
|    explained_variance    | 0.836         |
|    icm_loss              | 7.58e-10      |
|    intrinsic_reward_mean | 7.579942e-10  |
|    intrinsic_reward_std  | 5.2182647e-10 |
|    learning_rate         | 0.0002        |
|    loss                  | -0.00277      |
|    n_updates             | 196           |
|    policy_gradient_loss  | -0.0105       |
|    value_loss            | 0.138         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 172           |
|    ep_rew_mean           | 3.95          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 51            |
|    time_elapsed          | 4000          |
|    total_timesteps       | 417792        |
| train/                   |               |
|    approx_kl             | 0.006273209   |
|    clip_fraction         | 0.273         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.41         |
|    explained_variance    | 0.837         |
|    icm_loss              | 7.57e-10      |
|    intrinsic_reward_mean | 7.5673584e-10 |
|    intrinsic_reward_std  | 4.998161e-10  |
|    learning_rate         | 0.000199      |
|    loss                  | -0.00142      |
|    n_updates             | 200           |
|    policy_gradient_loss  | -0.0107       |
|    value_loss            | 0.116         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 179           |
|    ep_rew_mean           | 4.35          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 52            |
|    time_elapsed          | 4073          |
|    total_timesteps       | 425984        |
| train/                   |               |
|    approx_kl             | 0.006180957   |
|    clip_fraction         | 0.278         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.39         |
|    explained_variance    | 0.858         |
|    icm_loss              | 7.45e-10      |
|    intrinsic_reward_mean | 7.4477774e-10 |
|    intrinsic_reward_std  | 3.7187972e-10 |
|    learning_rate         | 0.000198      |
|    loss                  | 0.00364       |
|    n_updates             | 204           |
|    policy_gradient_loss  | -0.0108       |
|    value_loss            | 0.118         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 183           |
|    ep_rew_mean           | 4.36          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 53            |
|    time_elapsed          | 4142          |
|    total_timesteps       | 434176        |
| train/                   |               |
|    approx_kl             | 0.006023773   |
|    clip_fraction         | 0.281         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.43         |
|    explained_variance    | 0.862         |
|    icm_loss              | 7.38e-10      |
|    intrinsic_reward_mean | 7.3827466e-10 |
|    intrinsic_reward_std  | 4.5551019e-10 |
|    learning_rate         | 0.000197      |
|    loss                  | 0.0037        |
|    n_updates             | 208           |
|    policy_gradient_loss  | -0.0101       |
|    value_loss            | 0.117         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 180           |
|    ep_rew_mean           | 4.25          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 54            |
|    time_elapsed          | 4221          |
|    total_timesteps       | 442368        |
| train/                   |               |
|    approx_kl             | 0.00637825    |
|    clip_fraction         | 0.288         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.45         |
|    explained_variance    | 0.849         |
|    icm_loss              | 7.22e-10      |
|    intrinsic_reward_mean | 7.2213946e-10 |
|    intrinsic_reward_std  | 3.4022046e-10 |
|    learning_rate         | 0.000196      |
|    loss                  | -0.012        |
|    n_updates             | 212           |
|    policy_gradient_loss  | -0.0115       |
|    value_loss            | 0.103         |
--------------------------------------------
Eval num_timesteps=450000, episode_reward=3.70 +/- 1.28
Episode length: 195.50 +/- 67.00
-------------------------------------------
| eval/                    |              |
|    mean_ep_length        | 196          |
|    mean_reward           | 3.7          |
| time/                    |              |
|    total_timesteps       | 450000       |
| train/                   |              |
|    approx_kl             | 0.0060171327 |
|    clip_fraction         | 0.267        |
|    clip_range            | 0.1          |
|    entropy_loss          | -2.4         |
|    explained_variance    | 0.851        |
|    icm_loss              | 7.08e-10     |
|    intrinsic_reward_mean | 7.080855e-10 |
|    intrinsic_reward_std  | 3.464343e-10 |
|    learning_rate         | 0.000195     |
|    loss                  | 0.00268      |
|    n_updates             | 216          |
|    policy_gradient_loss  | -0.0111      |
|    value_loss            | 0.129        |
-------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 4.22     |
| time/              |          |
|    fps             | 104      |
|    iterations      | 55       |
|    time_elapsed    | 4310     |
|    total_timesteps | 450560   |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 176           |
|    ep_rew_mean           | 4.5           |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 56            |
|    time_elapsed          | 4384          |
|    total_timesteps       | 458752        |
| train/                   |               |
|    approx_kl             | 0.005612082   |
|    clip_fraction         | 0.268         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.38         |
|    explained_variance    | 0.872         |
|    icm_loss              | 6.98e-10      |
|    intrinsic_reward_mean | 6.9819744e-10 |
|    intrinsic_reward_std  | 3.6251782e-10 |
|    learning_rate         | 0.000194      |
|    loss                  | -0.00175      |
|    n_updates             | 220           |
|    policy_gradient_loss  | -0.011        |
|    value_loss            | 0.12          |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 183           |
|    ep_rew_mean           | 4.55          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 57            |
|    time_elapsed          | 4457          |
|    total_timesteps       | 466944        |
| train/                   |               |
|    approx_kl             | 0.0064780083  |
|    clip_fraction         | 0.3           |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.36         |
|    explained_variance    | 0.881         |
|    icm_loss              | 7.01e-10      |
|    intrinsic_reward_mean | 7.010834e-10  |
|    intrinsic_reward_std  | 3.7150977e-10 |
|    learning_rate         | 0.000193      |
|    loss                  | -0.0136       |
|    n_updates             | 224           |
|    policy_gradient_loss  | -0.0118       |
|    value_loss            | 0.0979        |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 181           |
|    ep_rew_mean           | 4.23          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 58            |
|    time_elapsed          | 4532          |
|    total_timesteps       | 475136        |
| train/                   |               |
|    approx_kl             | 0.0059518013  |
|    clip_fraction         | 0.273         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.36         |
|    explained_variance    | 0.858         |
|    icm_loss              | 6.91e-10      |
|    intrinsic_reward_mean | 6.9096673e-10 |
|    intrinsic_reward_std  | 3.954394e-10  |
|    learning_rate         | 0.000192      |
|    loss                  | -0.0159       |
|    n_updates             | 228           |
|    policy_gradient_loss  | -0.0122       |
|    value_loss            | 0.109         |
--------------------------------------------
-------------------------------------------
| rollout/                 |              |
|    ep_len_mean           | 172          |
|    ep_rew_mean           | 4.01         |
| time/                    |              |
|    fps                   | 104          |
|    iterations            | 59           |
|    time_elapsed          | 4610         |
|    total_timesteps       | 483328       |
| train/                   |              |
|    approx_kl             | 0.0072037675 |
|    clip_fraction         | 0.304        |
|    clip_range            | 0.1          |
|    entropy_loss          | -2.35        |
|    explained_variance    | 0.855        |
|    icm_loss              | 6.63e-10     |
|    intrinsic_reward_mean | 6.625991e-10 |
|    intrinsic_reward_std  | 3.78251e-10  |
|    learning_rate         | 0.000191     |
|    loss                  | -0.0239      |
|    n_updates             | 232          |
|    policy_gradient_loss  | -0.0108      |
|    value_loss            | 0.117        |
-------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 171           |
|    ep_rew_mean           | 4.24          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 60            |
|    time_elapsed          | 4685          |
|    total_timesteps       | 491520        |
| train/                   |               |
|    approx_kl             | 0.0057594324  |
|    clip_fraction         | 0.268         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.34         |
|    explained_variance    | 0.88          |
|    icm_loss              | 6.65e-10      |
|    intrinsic_reward_mean | 6.6533434e-10 |
|    intrinsic_reward_std  | 3.7038111e-10 |
|    learning_rate         | 0.00019       |
|    loss                  | -0.0073       |
|    n_updates             | 236           |
|    policy_gradient_loss  | -0.0115       |
|    value_loss            | 0.11          |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 178           |
|    ep_rew_mean           | 4.57          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 61            |
|    time_elapsed          | 4759          |
|    total_timesteps       | 499712        |
| train/                   |               |
|    approx_kl             | 0.0068327887  |
|    clip_fraction         | 0.301         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.32         |
|    explained_variance    | 0.867         |
|    icm_loss              | 6.64e-10      |
|    intrinsic_reward_mean | 6.642169e-10  |
|    intrinsic_reward_std  | 3.8129236e-10 |
|    learning_rate         | 0.000189      |
|    loss                  | -0.00433      |
|    n_updates             | 240           |
|    policy_gradient_loss  | -0.0113       |
|    value_loss            | 0.122         |
--------------------------------------------
Eval num_timesteps=500000, episode_reward=3.00 +/- 1.30
Episode length: 189.20 +/- 44.01
-------------------------------------------
| eval/                    |              |
|    mean_ep_length        | 189          |
|    mean_reward           | 3            |
| time/                    |              |
|    total_timesteps       | 500000       |
| train/                   |              |
|    approx_kl             | 0.006155026  |
|    clip_fraction         | 0.281        |
|    clip_range            | 0.1          |
|    entropy_loss          | -2.33        |
|    explained_variance    | 0.894        |
|    icm_loss              | 6.52e-10     |
|    intrinsic_reward_mean | 6.516695e-10 |
|    intrinsic_reward_std  | 2.916824e-10 |
|    learning_rate         | 0.000188     |
|    loss                  | -0.0123      |
|    n_updates             | 244          |
|    policy_gradient_loss  | -0.011       |
|    value_loss            | 0.102        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 4.38     |
| time/              |          |
|    fps             | 104      |
|    iterations      | 62       |
|    time_elapsed    | 4854     |
|    total_timesteps | 507904   |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 172           |
|    ep_rew_mean           | 4.49          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 63            |
|    time_elapsed          | 4926          |
|    total_timesteps       | 516096        |
| train/                   |               |
|    approx_kl             | 0.006248896   |
|    clip_fraction         | 0.283         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.3          |
|    explained_variance    | 0.868         |
|    icm_loss              | 6.42e-10      |
|    intrinsic_reward_mean | 6.4249095e-10 |
|    intrinsic_reward_std  | 4.198875e-10  |
|    learning_rate         | 0.000187      |
|    loss                  | 0.0133        |
|    n_updates             | 248           |
|    policy_gradient_loss  | -0.0101       |
|    value_loss            | 0.125         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 178           |
|    ep_rew_mean           | 4.84          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 64            |
|    time_elapsed          | 5004          |
|    total_timesteps       | 524288        |
| train/                   |               |
|    approx_kl             | 0.006952132   |
|    clip_fraction         | 0.292         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.31         |
|    explained_variance    | 0.857         |
|    icm_loss              | 6.38e-10      |
|    intrinsic_reward_mean | 6.378226e-10  |
|    intrinsic_reward_std  | 2.7659194e-10 |
|    learning_rate         | 0.000185      |
|    loss                  | -0.00764      |
|    n_updates             | 252           |
|    policy_gradient_loss  | -0.011        |
|    value_loss            | 0.127         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 175           |
|    ep_rew_mean           | 4.85          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 65            |
|    time_elapsed          | 5080          |
|    total_timesteps       | 532480        |
| train/                   |               |
|    approx_kl             | 0.0061441828  |
|    clip_fraction         | 0.289         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.29         |
|    explained_variance    | 0.867         |
|    icm_loss              | 6.36e-10      |
|    intrinsic_reward_mean | 6.3552846e-10 |
|    intrinsic_reward_std  | 3.0785907e-10 |
|    learning_rate         | 0.000184      |
|    loss                  | -0.00369      |
|    n_updates             | 256           |
|    policy_gradient_loss  | -0.0105       |
|    value_loss            | 0.128         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 175           |
|    ep_rew_mean           | 4.81          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 66            |
|    time_elapsed          | 5155          |
|    total_timesteps       | 540672        |
| train/                   |               |
|    approx_kl             | 0.006594175   |
|    clip_fraction         | 0.284         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.31         |
|    explained_variance    | 0.853         |
|    icm_loss              | 6.2e-10       |
|    intrinsic_reward_mean | 6.195238e-10  |
|    intrinsic_reward_std  | 3.2006747e-10 |
|    learning_rate         | 0.000183      |
|    loss                  | -0.00167      |
|    n_updates             | 260           |
|    policy_gradient_loss  | -0.011        |
|    value_loss            | 0.124         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 178           |
|    ep_rew_mean           | 4.54          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 67            |
|    time_elapsed          | 5230          |
|    total_timesteps       | 548864        |
| train/                   |               |
|    approx_kl             | 0.005457322   |
|    clip_fraction         | 0.264         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.34         |
|    explained_variance    | 0.857         |
|    icm_loss              | 6.23e-10      |
|    intrinsic_reward_mean | 6.230733e-10  |
|    intrinsic_reward_std  | 2.7452268e-10 |
|    learning_rate         | 0.000182      |
|    loss                  | -0.00137      |
|    n_updates             | 264           |
|    policy_gradient_loss  | -0.0107       |
|    value_loss            | 0.119         |
--------------------------------------------
Eval num_timesteps=550000, episode_reward=4.20 +/- 1.45
Episode length: 167.60 +/- 32.77
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 168           |
|    mean_reward           | 4.2           |
| time/                    |               |
|    total_timesteps       | 550000        |
| train/                   |               |
|    approx_kl             | 0.0063936803  |
|    clip_fraction         | 0.294         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.32         |
|    explained_variance    | 0.87          |
|    icm_loss              | 6.01e-10      |
|    intrinsic_reward_mean | 6.010946e-10  |
|    intrinsic_reward_std  | 3.1337033e-10 |
|    learning_rate         | 0.000181      |
|    loss                  | -0.0136       |
|    n_updates             | 268           |
|    policy_gradient_loss  | -0.0108       |
|    value_loss            | 0.109         |
--------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 4.42     |
| time/              |          |
|    fps             | 104      |
|    iterations      | 68       |
|    time_elapsed    | 5318     |
|    total_timesteps | 557056   |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 186           |
|    ep_rew_mean           | 4.5           |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 69            |
|    time_elapsed          | 5396          |
|    total_timesteps       | 565248        |
| train/                   |               |
|    approx_kl             | 0.0060820035  |
|    clip_fraction         | 0.264         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.36         |
|    explained_variance    | 0.849         |
|    icm_loss              | 6.12e-10      |
|    intrinsic_reward_mean | 6.1188415e-10 |
|    intrinsic_reward_std  | 3.037775e-10  |
|    learning_rate         | 0.00018       |
|    loss                  | 0.000655      |
|    n_updates             | 272           |
|    policy_gradient_loss  | -0.00915      |
|    value_loss            | 0.131         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 179           |
|    ep_rew_mean           | 4.57          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 70            |
|    time_elapsed          | 5472          |
|    total_timesteps       | 573440        |
| train/                   |               |
|    approx_kl             | 0.0063250754  |
|    clip_fraction         | 0.302         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.31         |
|    explained_variance    | 0.861         |
|    icm_loss              | 5.99e-10      |
|    intrinsic_reward_mean | 5.98642e-10   |
|    intrinsic_reward_std  | 3.0546798e-10 |
|    learning_rate         | 0.000179      |
|    loss                  | 0.00557       |
|    n_updates             | 276           |
|    policy_gradient_loss  | -0.0126       |
|    value_loss            | 0.12          |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 176           |
|    ep_rew_mean           | 4.55          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 71            |
|    time_elapsed          | 5548          |
|    total_timesteps       | 581632        |
| train/                   |               |
|    approx_kl             | 0.006236036   |
|    clip_fraction         | 0.295         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.32         |
|    explained_variance    | 0.851         |
|    icm_loss              | 5.92e-10      |
|    intrinsic_reward_mean | 5.91967e-10   |
|    intrinsic_reward_std  | 2.4994234e-10 |
|    learning_rate         | 0.000178      |
|    loss                  | 0.00584       |
|    n_updates             | 280           |
|    policy_gradient_loss  | -0.0109       |
|    value_loss            | 0.122         |
--------------------------------------------
-------------------------------------------
| rollout/                 |              |
|    ep_len_mean           | 176          |
|    ep_rew_mean           | 4.74         |
| time/                    |              |
|    fps                   | 104          |
|    iterations            | 72           |
|    time_elapsed          | 5624         |
|    total_timesteps       | 589824       |
| train/                   |              |
|    approx_kl             | 0.006947008  |
|    clip_fraction         | 0.278        |
|    clip_range            | 0.1          |
|    entropy_loss          | -2.32        |
|    explained_variance    | 0.875        |
|    icm_loss              | 5.85e-10     |
|    intrinsic_reward_mean | 5.845159e-10 |
|    intrinsic_reward_std  | 2.794708e-10 |
|    learning_rate         | 0.000177     |
|    loss                  | -0.0236      |
|    n_updates             | 284          |
|    policy_gradient_loss  | -0.0124      |
|    value_loss            | 0.11         |
-------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 181           |
|    ep_rew_mean           | 4.88          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 73            |
|    time_elapsed          | 5696          |
|    total_timesteps       | 598016        |
| train/                   |               |
|    approx_kl             | 0.0057705482  |
|    clip_fraction         | 0.281         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.31         |
|    explained_variance    | 0.865         |
|    icm_loss              | 5.82e-10      |
|    intrinsic_reward_mean | 5.824691e-10  |
|    intrinsic_reward_std  | 2.4823957e-10 |
|    learning_rate         | 0.000176      |
|    loss                  | 0.00612       |
|    n_updates             | 288           |
|    policy_gradient_loss  | -0.00987      |
|    value_loss            | 0.125         |
--------------------------------------------
Eval num_timesteps=600000, episode_reward=3.50 +/- 0.66
Episode length: 182.50 +/- 36.52
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 182           |
|    mean_reward           | 3.5           |
| time/                    |               |
|    total_timesteps       | 600000        |
| train/                   |               |
|    approx_kl             | 0.0067498847  |
|    clip_fraction         | 0.292         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.29         |
|    explained_variance    | 0.855         |
|    icm_loss              | 5.79e-10      |
|    intrinsic_reward_mean | 5.785349e-10  |
|    intrinsic_reward_std  | 3.3185962e-10 |
|    learning_rate         | 0.000175      |
|    loss                  | -0.0114       |
|    n_updates             | 292           |
|    policy_gradient_loss  | -0.0127       |
|    value_loss            | 0.122         |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 181      |
|    ep_rew_mean     | 5        |
| time/              |          |
|    fps             | 104      |
|    iterations      | 74       |
|    time_elapsed    | 5788     |
|    total_timesteps | 606208   |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 182           |
|    ep_rew_mean           | 4.97          |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 75            |
|    time_elapsed          | 5862          |
|    total_timesteps       | 614400        |
| train/                   |               |
|    approx_kl             | 0.0062958403  |
|    clip_fraction         | 0.295         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.27         |
|    explained_variance    | 0.85          |
|    icm_loss              | 5.75e-10      |
|    intrinsic_reward_mean | 5.7507515e-10 |
|    intrinsic_reward_std  | 2.9414945e-10 |
|    learning_rate         | 0.000174      |
|    loss                  | 0.000258      |
|    n_updates             | 296           |
|    policy_gradient_loss  | -0.00953      |
|    value_loss            | 0.14          |
--------------------------------------------
-------------------------------------------
| rollout/                 |              |
|    ep_len_mean           | 185          |
|    ep_rew_mean           | 5.03         |
| time/                    |              |
|    fps                   | 104          |
|    iterations            | 76           |
|    time_elapsed          | 5935         |
|    total_timesteps       | 622592       |
| train/                   |              |
|    approx_kl             | 0.0065782242 |
|    clip_fraction         | 0.291        |
|    clip_range            | 0.1          |
|    entropy_loss          | -2.32        |
|    explained_variance    | 0.883        |
|    icm_loss              | 5.61e-10     |
|    intrinsic_reward_mean | 5.612626e-10 |
|    intrinsic_reward_std  | 2.699048e-10 |
|    learning_rate         | 0.000173     |
|    loss                  | 0.00343      |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.0111      |
|    value_loss            | 0.115        |
-------------------------------------------
-------------------------------------------
| rollout/                 |              |
|    ep_len_mean           | 187          |
|    ep_rew_mean           | 4.83         |
| time/                    |              |
|    fps                   | 105          |
|    iterations            | 77           |
|    time_elapsed          | 6005         |
|    total_timesteps       | 630784       |
| train/                   |              |
|    approx_kl             | 0.006165376  |
|    clip_fraction         | 0.288        |
|    clip_range            | 0.1          |
|    entropy_loss          | -2.29        |
|    explained_variance    | 0.876        |
|    icm_loss              | 5.63e-10     |
|    intrinsic_reward_mean | 5.631202e-10 |
|    intrinsic_reward_std  | 2.429517e-10 |
|    learning_rate         | 0.000172     |
|    loss                  | -0.0142      |
|    n_updates             | 304          |
|    policy_gradient_loss  | -0.0112      |
|    value_loss            | 0.113        |
-------------------------------------------
-------------------------------------------
| rollout/                 |              |
|    ep_len_mean           | 191          |
|    ep_rew_mean           | 5            |
| time/                    |              |
|    fps                   | 105          |
|    iterations            | 78           |
|    time_elapsed          | 6078         |
|    total_timesteps       | 638976       |
| train/                   |              |
|    approx_kl             | 0.006383382  |
|    clip_fraction         | 0.294        |
|    clip_range            | 0.1          |
|    entropy_loss          | -2.35        |
|    explained_variance    | 0.876        |
|    icm_loss              | 5.62e-10     |
|    intrinsic_reward_mean | 5.618413e-10 |
|    intrinsic_reward_std  | 2.609298e-10 |
|    learning_rate         | 0.000171     |
|    loss                  | -0.000704    |
|    n_updates             | 308          |
|    policy_gradient_loss  | -0.011       |
|    value_loss            | 0.118        |
-------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 190           |
|    ep_rew_mean           | 4.83          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 79            |
|    time_elapsed          | 6152          |
|    total_timesteps       | 647168        |
| train/                   |               |
|    approx_kl             | 0.005726415   |
|    clip_fraction         | 0.263         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.35         |
|    explained_variance    | 0.873         |
|    icm_loss              | 5.52e-10      |
|    intrinsic_reward_mean | 5.515247e-10  |
|    intrinsic_reward_std  | 2.5625357e-10 |
|    learning_rate         | 0.00017       |
|    loss                  | -0.0158       |
|    n_updates             | 312           |
|    policy_gradient_loss  | -0.0115       |
|    value_loss            | 0.126         |
--------------------------------------------
Eval num_timesteps=650000, episode_reward=3.00 +/- 0.83
Episode length: 167.60 +/- 42.45
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 168           |
|    mean_reward           | 3             |
| time/                    |               |
|    total_timesteps       | 650000        |
| train/                   |               |
|    approx_kl             | 0.006368473   |
|    clip_fraction         | 0.273         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.31         |
|    explained_variance    | 0.874         |
|    icm_loss              | 5.5e-10       |
|    intrinsic_reward_mean | 5.499037e-10  |
|    intrinsic_reward_std  | 2.3064424e-10 |
|    learning_rate         | 0.000169      |
|    loss                  | 0.00855       |
|    n_updates             | 316           |
|    policy_gradient_loss  | -0.0108       |
|    value_loss            | 0.131         |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 4.74     |
| time/              |          |
|    fps             | 105      |
|    iterations      | 80       |
|    time_elapsed    | 6237     |
|    total_timesteps | 655360   |
---------------------------------
-------------------------------------------
| rollout/                 |              |
|    ep_len_mean           | 188          |
|    ep_rew_mean           | 5.01         |
| time/                    |              |
|    fps                   | 105          |
|    iterations            | 81           |
|    time_elapsed          | 6312         |
|    total_timesteps       | 663552       |
| train/                   |              |
|    approx_kl             | 0.0066293194 |
|    clip_fraction         | 0.29         |
|    clip_range            | 0.1          |
|    entropy_loss          | -2.34        |
|    explained_variance    | 0.87         |
|    icm_loss              | 5.46e-10     |
|    intrinsic_reward_mean | 5.461777e-10 |
|    intrinsic_reward_std  | 2.63415e-10  |
|    learning_rate         | 0.000168     |
|    loss                  | -0.00568     |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.0121      |
|    value_loss            | 0.113        |
-------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 182           |
|    ep_rew_mean           | 4.98          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 82            |
|    time_elapsed          | 6384          |
|    total_timesteps       | 671744        |
| train/                   |               |
|    approx_kl             | 0.0063465484  |
|    clip_fraction         | 0.291         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.3          |
|    explained_variance    | 0.871         |
|    icm_loss              | 5.37e-10      |
|    intrinsic_reward_mean | 5.3689625e-10 |
|    intrinsic_reward_std  | 2.4312266e-10 |
|    learning_rate         | 0.000167      |
|    loss                  | -0.00347      |
|    n_updates             | 324           |
|    policy_gradient_loss  | -0.0111       |
|    value_loss            | 0.121         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 182           |
|    ep_rew_mean           | 4.99          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 83            |
|    time_elapsed          | 6457          |
|    total_timesteps       | 679936        |
| train/                   |               |
|    approx_kl             | 0.007404616   |
|    clip_fraction         | 0.291         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.33         |
|    explained_variance    | 0.863         |
|    icm_loss              | 5.37e-10      |
|    intrinsic_reward_mean | 5.374492e-10  |
|    intrinsic_reward_std  | 2.2536245e-10 |
|    learning_rate         | 0.000166      |
|    loss                  | -0.0057       |
|    n_updates             | 328           |
|    policy_gradient_loss  | -0.0115       |
|    value_loss            | 0.122         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 181           |
|    ep_rew_mean           | 4.91          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 84            |
|    time_elapsed          | 6535          |
|    total_timesteps       | 688128        |
| train/                   |               |
|    approx_kl             | 0.006247514   |
|    clip_fraction         | 0.279         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.32         |
|    explained_variance    | 0.871         |
|    icm_loss              | 5.28e-10      |
|    intrinsic_reward_mean | 5.281936e-10  |
|    intrinsic_reward_std  | 2.0277065e-10 |
|    learning_rate         | 0.000165      |
|    loss                  | 0.000133      |
|    n_updates             | 332           |
|    policy_gradient_loss  | -0.0122       |
|    value_loss            | 0.123         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 175           |
|    ep_rew_mean           | 4.79          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 85            |
|    time_elapsed          | 6611          |
|    total_timesteps       | 696320        |
| train/                   |               |
|    approx_kl             | 0.0067083808  |
|    clip_fraction         | 0.29          |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.3          |
|    explained_variance    | 0.873         |
|    icm_loss              | 5.3e-10       |
|    intrinsic_reward_mean | 5.2992577e-10 |
|    intrinsic_reward_std  | 2.3605437e-10 |
|    learning_rate         | 0.000164      |
|    loss                  | 0.0143        |
|    n_updates             | 336           |
|    policy_gradient_loss  | -0.00862      |
|    value_loss            | 0.127         |
--------------------------------------------
Eval num_timesteps=700000, episode_reward=3.60 +/- 1.12
Episode length: 191.30 +/- 54.52
-------------------------------------------
| eval/                    |              |
|    mean_ep_length        | 191          |
|    mean_reward           | 3.6          |
| time/                    |              |
|    total_timesteps       | 700000       |
| train/                   |              |
|    approx_kl             | 0.0066952445 |
|    clip_fraction         | 0.291        |
|    clip_range            | 0.1          |
|    entropy_loss          | -2.32        |
|    explained_variance    | 0.877        |
|    icm_loss              | 5.27e-10     |
|    intrinsic_reward_mean | 5.273266e-10 |
|    intrinsic_reward_std  | 2.41807e-10  |
|    learning_rate         | 0.000163     |
|    loss                  | -0.0125      |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.0112      |
|    value_loss            | 0.127        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 4.87     |
| time/              |          |
|    fps             | 105      |
|    iterations      | 86       |
|    time_elapsed    | 6701     |
|    total_timesteps | 704512   |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 181           |
|    ep_rew_mean           | 4.94          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 87            |
|    time_elapsed          | 6773          |
|    total_timesteps       | 712704        |
| train/                   |               |
|    approx_kl             | 0.007333399   |
|    clip_fraction         | 0.29          |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.28         |
|    explained_variance    | 0.847         |
|    icm_loss              | 5.22e-10      |
|    intrinsic_reward_mean | 5.221257e-10  |
|    intrinsic_reward_std  | 2.4471958e-10 |
|    learning_rate         | 0.000162      |
|    loss                  | 0.00461       |
|    n_updates             | 344           |
|    policy_gradient_loss  | -0.0112       |
|    value_loss            | 0.15          |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 183           |
|    ep_rew_mean           | 5.05          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 88            |
|    time_elapsed          | 6847          |
|    total_timesteps       | 720896        |
| train/                   |               |
|    approx_kl             | 0.006322558   |
|    clip_fraction         | 0.291         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.29         |
|    explained_variance    | 0.854         |
|    icm_loss              | 5.16e-10      |
|    intrinsic_reward_mean | 5.16313e-10   |
|    intrinsic_reward_std  | 2.0716259e-10 |
|    learning_rate         | 0.000161      |
|    loss                  | -0.0015       |
|    n_updates             | 348           |
|    policy_gradient_loss  | -0.0117       |
|    value_loss            | 0.127         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 186           |
|    ep_rew_mean           | 5.11          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 89            |
|    time_elapsed          | 6918          |
|    total_timesteps       | 729088        |
| train/                   |               |
|    approx_kl             | 0.006706391   |
|    clip_fraction         | 0.295         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.3          |
|    explained_variance    | 0.863         |
|    icm_loss              | 5.12e-10      |
|    intrinsic_reward_mean | 5.118401e-10  |
|    intrinsic_reward_std  | 1.8704788e-10 |
|    learning_rate         | 0.00016       |
|    loss                  | 0.00367       |
|    n_updates             | 352           |
|    policy_gradient_loss  | -0.0124       |
|    value_loss            | 0.132         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 185           |
|    ep_rew_mean           | 5.07          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 90            |
|    time_elapsed          | 6993          |
|    total_timesteps       | 737280        |
| train/                   |               |
|    approx_kl             | 0.006481885   |
|    clip_fraction         | 0.302         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.32         |
|    explained_variance    | 0.876         |
|    icm_loss              | 5.11e-10      |
|    intrinsic_reward_mean | 5.112719e-10  |
|    intrinsic_reward_std  | 2.0594401e-10 |
|    learning_rate         | 0.000159      |
|    loss                  | 0.00443       |
|    n_updates             | 356           |
|    policy_gradient_loss  | -0.0129       |
|    value_loss            | 0.121         |
--------------------------------------------
-------------------------------------------
| rollout/                 |              |
|    ep_len_mean           | 184          |
|    ep_rew_mean           | 4.97         |
| time/                    |              |
|    fps                   | 105          |
|    iterations            | 91           |
|    time_elapsed          | 7069         |
|    total_timesteps       | 745472       |
| train/                   |              |
|    approx_kl             | 0.0062538395 |
|    clip_fraction         | 0.279        |
|    clip_range            | 0.1          |
|    entropy_loss          | -2.31        |
|    explained_variance    | 0.886        |
|    icm_loss              | 5.14e-10     |
|    intrinsic_reward_mean | 5.141137e-10 |
|    intrinsic_reward_std  | 2.582726e-10 |
|    learning_rate         | 0.000158     |
|    loss                  | -0.0113      |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.0109      |
|    value_loss            | 0.108        |
-------------------------------------------
Eval num_timesteps=750000, episode_reward=4.50 +/- 1.11
Episode length: 171.30 +/- 38.81
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 171           |
|    mean_reward           | 4.5           |
| time/                    |               |
|    total_timesteps       | 750000        |
| train/                   |               |
|    approx_kl             | 0.006574377   |
|    clip_fraction         | 0.288         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.29         |
|    explained_variance    | 0.839         |
|    icm_loss              | 5.07e-10      |
|    intrinsic_reward_mean | 5.0656956e-10 |
|    intrinsic_reward_std  | 1.9445967e-10 |
|    learning_rate         | 0.000157      |
|    loss                  | 0.00309       |
|    n_updates             | 364           |
|    policy_gradient_loss  | -0.0116       |
|    value_loss            | 0.135         |
--------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 4.97     |
| time/              |          |
|    fps             | 105      |
|    iterations      | 92       |
|    time_elapsed    | 7161     |
|    total_timesteps | 753664   |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 183           |
|    ep_rew_mean           | 5.07          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 93            |
|    time_elapsed          | 7233          |
|    total_timesteps       | 761856        |
| train/                   |               |
|    approx_kl             | 0.0067586536  |
|    clip_fraction         | 0.3           |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.3          |
|    explained_variance    | 0.869         |
|    icm_loss              | 5.01e-10      |
|    intrinsic_reward_mean | 5.0139515e-10 |
|    intrinsic_reward_std  | 1.9546019e-10 |
|    learning_rate         | 0.000156      |
|    loss                  | -0.000782     |
|    n_updates             | 368           |
|    policy_gradient_loss  | -0.0123       |
|    value_loss            | 0.126         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 181           |
|    ep_rew_mean           | 4.96          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 94            |
|    time_elapsed          | 7307          |
|    total_timesteps       | 770048        |
| train/                   |               |
|    approx_kl             | 0.006883106   |
|    clip_fraction         | 0.295         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.3          |
|    explained_variance    | 0.87          |
|    icm_loss              | 4.96e-10      |
|    intrinsic_reward_mean | 4.9639187e-10 |
|    intrinsic_reward_std  | 2.0678256e-10 |
|    learning_rate         | 0.000155      |
|    loss                  | -0.0139       |
|    n_updates             | 372           |
|    policy_gradient_loss  | -0.0129       |
|    value_loss            | 0.122         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 191           |
|    ep_rew_mean           | 5.15          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 95            |
|    time_elapsed          | 7377          |
|    total_timesteps       | 778240        |
| train/                   |               |
|    approx_kl             | 0.0062433067  |
|    clip_fraction         | 0.289         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.26         |
|    explained_variance    | 0.883         |
|    icm_loss              | 5e-10         |
|    intrinsic_reward_mean | 4.996869e-10  |
|    intrinsic_reward_std  | 1.7420715e-10 |
|    learning_rate         | 0.000154      |
|    loss                  | 0.0063        |
|    n_updates             | 376           |
|    policy_gradient_loss  | -0.0116       |
|    value_loss            | 0.12          |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 185           |
|    ep_rew_mean           | 5.24          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 96            |
|    time_elapsed          | 7452          |
|    total_timesteps       | 786432        |
| train/                   |               |
|    approx_kl             | 0.006297637   |
|    clip_fraction         | 0.295         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.29         |
|    explained_variance    | 0.875         |
|    icm_loss              | 4.92e-10      |
|    intrinsic_reward_mean | 4.9208027e-10 |
|    intrinsic_reward_std  | 2.3007132e-10 |
|    learning_rate         | 0.000153      |
|    loss                  | -0.00512      |
|    n_updates             | 380           |
|    policy_gradient_loss  | -0.0124       |
|    value_loss            | 0.112         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 176           |
|    ep_rew_mean           | 5.2           |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 97            |
|    time_elapsed          | 7530          |
|    total_timesteps       | 794624        |
| train/                   |               |
|    approx_kl             | 0.0067431116  |
|    clip_fraction         | 0.295         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.24         |
|    explained_variance    | 0.875         |
|    icm_loss              | 4.9e-10       |
|    intrinsic_reward_mean | 4.900472e-10  |
|    intrinsic_reward_std  | 1.7927225e-10 |
|    learning_rate         | 0.000152      |
|    loss                  | -0.00383      |
|    n_updates             | 384           |
|    policy_gradient_loss  | -0.0119       |
|    value_loss            | 0.125         |
--------------------------------------------
Eval num_timesteps=800000, episode_reward=3.80 +/- 0.64
Episode length: 168.80 +/- 42.33
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 169           |
|    mean_reward           | 3.8           |
| time/                    |               |
|    total_timesteps       | 800000        |
| train/                   |               |
|    approx_kl             | 0.00649163    |
|    clip_fraction         | 0.288         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.21         |
|    explained_variance    | 0.855         |
|    icm_loss              | 4.88e-10      |
|    intrinsic_reward_mean | 4.882693e-10  |
|    intrinsic_reward_std  | 2.3779329e-10 |
|    learning_rate         | 0.000151      |
|    loss                  | -0.00423      |
|    n_updates             | 388           |
|    policy_gradient_loss  | -0.0101       |
|    value_loss            | 0.145         |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 5.16     |
| time/              |          |
|    fps             | 105      |
|    iterations      | 98       |
|    time_elapsed    | 7620     |
|    total_timesteps | 802816   |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 180           |
|    ep_rew_mean           | 5.04          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 99            |
|    time_elapsed          | 7695          |
|    total_timesteps       | 811008        |
| train/                   |               |
|    approx_kl             | 0.0068111224  |
|    clip_fraction         | 0.298         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.25         |
|    explained_variance    | 0.866         |
|    icm_loss              | 4.85e-10      |
|    intrinsic_reward_mean | 4.852345e-10  |
|    intrinsic_reward_std  | 1.7629034e-10 |
|    learning_rate         | 0.00015       |
|    loss                  | 0.00386       |
|    n_updates             | 392           |
|    policy_gradient_loss  | -0.0123       |
|    value_loss            | 0.122         |
--------------------------------------------
-------------------------------------------
| rollout/                 |              |
|    ep_len_mean           | 178          |
|    ep_rew_mean           | 5.06         |
| time/                    |              |
|    fps                   | 105          |
|    iterations            | 100          |
|    time_elapsed          | 7772         |
|    total_timesteps       | 819200       |
| train/                   |              |
|    approx_kl             | 0.006025075  |
|    clip_fraction         | 0.271        |
|    clip_range            | 0.1          |
|    entropy_loss          | -2.25        |
|    explained_variance    | 0.854        |
|    icm_loss              | 4.85e-10     |
|    intrinsic_reward_mean | 4.854508e-10 |
|    intrinsic_reward_std  | 2.475549e-10 |
|    learning_rate         | 0.000149     |
|    loss                  | 0.000318     |
|    n_updates             | 396          |
|    policy_gradient_loss  | -0.0108      |
|    value_loss            | 0.14         |
-------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 175           |
|    ep_rew_mean           | 5.07          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 101           |
|    time_elapsed          | 7847          |
|    total_timesteps       | 827392        |
| train/                   |               |
|    approx_kl             | 0.0072628064  |
|    clip_fraction         | 0.299         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.16         |
|    explained_variance    | 0.846         |
|    icm_loss              | 4.73e-10      |
|    intrinsic_reward_mean | 4.7275966e-10 |
|    intrinsic_reward_std  | 1.9862612e-10 |
|    learning_rate         | 0.000148      |
|    loss                  | 0.0042        |
|    n_updates             | 400           |
|    policy_gradient_loss  | -0.0108       |
|    value_loss            | 0.15          |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 178           |
|    ep_rew_mean           | 5.19          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 102           |
|    time_elapsed          | 7923          |
|    total_timesteps       | 835584        |
| train/                   |               |
|    approx_kl             | 0.0063713985  |
|    clip_fraction         | 0.294         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.18         |
|    explained_variance    | 0.862         |
|    icm_loss              | 4.79e-10      |
|    intrinsic_reward_mean | 4.792149e-10  |
|    intrinsic_reward_std  | 1.9413565e-10 |
|    learning_rate         | 0.000147      |
|    loss                  | 0.00555       |
|    n_updates             | 404           |
|    policy_gradient_loss  | -0.0109       |
|    value_loss            | 0.133         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 180           |
|    ep_rew_mean           | 5.44          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 103           |
|    time_elapsed          | 7998          |
|    total_timesteps       | 843776        |
| train/                   |               |
|    approx_kl             | 0.0065617533  |
|    clip_fraction         | 0.29          |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.19         |
|    explained_variance    | 0.862         |
|    icm_loss              | 4.72e-10      |
|    intrinsic_reward_mean | 4.720879e-10  |
|    intrinsic_reward_std  | 1.8906661e-10 |
|    learning_rate         | 0.000146      |
|    loss                  | 0.000326      |
|    n_updates             | 408           |
|    policy_gradient_loss  | -0.0112       |
|    value_loss            | 0.141         |
--------------------------------------------
Eval num_timesteps=850000, episode_reward=5.00 +/- 0.83
Episode length: 189.00 +/- 40.39
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 189           |
|    mean_reward           | 5             |
| time/                    |               |
|    total_timesteps       | 850000        |
| train/                   |               |
|    approx_kl             | 0.0066553885  |
|    clip_fraction         | 0.27          |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.17         |
|    explained_variance    | 0.89          |
|    icm_loss              | 4.73e-10      |
|    intrinsic_reward_mean | 4.727353e-10  |
|    intrinsic_reward_std  | 1.5824912e-10 |
|    learning_rate         | 0.000145      |
|    loss                  | -0.0128       |
|    n_updates             | 412           |
|    policy_gradient_loss  | -0.0127       |
|    value_loss            | 0.119         |
--------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 5.67     |
| time/              |          |
|    fps             | 105      |
|    iterations      | 104      |
|    time_elapsed    | 8086     |
|    total_timesteps | 851968   |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 181           |
|    ep_rew_mean           | 5.36          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 105           |
|    time_elapsed          | 8163          |
|    total_timesteps       | 860160        |
| train/                   |               |
|    approx_kl             | 0.007062448   |
|    clip_fraction         | 0.304         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.22         |
|    explained_variance    | 0.868         |
|    icm_loss              | 4.65e-10      |
|    intrinsic_reward_mean | 4.6503257e-10 |
|    intrinsic_reward_std  | 2.1133303e-10 |
|    learning_rate         | 0.000144      |
|    loss                  | 0.0078        |
|    n_updates             | 416           |
|    policy_gradient_loss  | -0.0107       |
|    value_loss            | 0.143         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 180           |
|    ep_rew_mean           | 5.22          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 106           |
|    time_elapsed          | 8239          |
|    total_timesteps       | 868352        |
| train/                   |               |
|    approx_kl             | 0.0067032054  |
|    clip_fraction         | 0.288         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.16         |
|    explained_variance    | 0.849         |
|    icm_loss              | 4.64e-10      |
|    intrinsic_reward_mean | 4.6364557e-10 |
|    intrinsic_reward_std  | 1.9935395e-10 |
|    learning_rate         | 0.000142      |
|    loss                  | 0.0247        |
|    n_updates             | 420           |
|    policy_gradient_loss  | -0.0107       |
|    value_loss            | 0.151         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 180           |
|    ep_rew_mean           | 5.34          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 107           |
|    time_elapsed          | 8312          |
|    total_timesteps       | 876544        |
| train/                   |               |
|    approx_kl             | 0.00689438    |
|    clip_fraction         | 0.298         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.17         |
|    explained_variance    | 0.863         |
|    icm_loss              | 4.62e-10      |
|    intrinsic_reward_mean | 4.6178983e-10 |
|    intrinsic_reward_std  | 2.0579781e-10 |
|    learning_rate         | 0.000141      |
|    loss                  | 0.0246        |
|    n_updates             | 424           |
|    policy_gradient_loss  | -0.0111       |
|    value_loss            | 0.145         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 180           |
|    ep_rew_mean           | 5.42          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 108           |
|    time_elapsed          | 8388          |
|    total_timesteps       | 884736        |
| train/                   |               |
|    approx_kl             | 0.0068556936  |
|    clip_fraction         | 0.294         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.21         |
|    explained_variance    | 0.845         |
|    icm_loss              | 4.56e-10      |
|    intrinsic_reward_mean | 4.5555412e-10 |
|    intrinsic_reward_std  | 1.6540996e-10 |
|    learning_rate         | 0.00014       |
|    loss                  | 0.0135        |
|    n_updates             | 428           |
|    policy_gradient_loss  | -0.012        |
|    value_loss            | 0.146         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 179           |
|    ep_rew_mean           | 5.24          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 109           |
|    time_elapsed          | 8460          |
|    total_timesteps       | 892928        |
| train/                   |               |
|    approx_kl             | 0.007102579   |
|    clip_fraction         | 0.306         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.2          |
|    explained_variance    | 0.853         |
|    icm_loss              | 4.59e-10      |
|    intrinsic_reward_mean | 4.590717e-10  |
|    intrinsic_reward_std  | 2.0457061e-10 |
|    learning_rate         | 0.000139      |
|    loss                  | 0.00482       |
|    n_updates             | 432           |
|    policy_gradient_loss  | -0.0116       |
|    value_loss            | 0.141         |
--------------------------------------------
Eval num_timesteps=900000, episode_reward=4.40 +/- 1.10
Episode length: 179.10 +/- 42.76
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 179           |
|    mean_reward           | 4.4           |
| time/                    |               |
|    total_timesteps       | 900000        |
| train/                   |               |
|    approx_kl             | 0.006306534   |
|    clip_fraction         | 0.306         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.19         |
|    explained_variance    | 0.836         |
|    icm_loss              | 4.56e-10      |
|    intrinsic_reward_mean | 4.556994e-10  |
|    intrinsic_reward_std  | 1.5480386e-10 |
|    learning_rate         | 0.000138      |
|    loss                  | -0.00439      |
|    n_updates             | 436           |
|    policy_gradient_loss  | -0.0113       |
|    value_loss            | 0.145         |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 5.02     |
| time/              |          |
|    fps             | 105      |
|    iterations      | 110      |
|    time_elapsed    | 8555     |
|    total_timesteps | 901120   |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 178           |
|    ep_rew_mean           | 5.28          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 111           |
|    time_elapsed          | 8624          |
|    total_timesteps       | 909312        |
| train/                   |               |
|    approx_kl             | 0.0065012975  |
|    clip_fraction         | 0.285         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.16         |
|    explained_variance    | 0.827         |
|    icm_loss              | 4.57e-10      |
|    intrinsic_reward_mean | 4.5722193e-10 |
|    intrinsic_reward_std  | 2.095924e-10  |
|    learning_rate         | 0.000137      |
|    loss                  | 0.018         |
|    n_updates             | 440           |
|    policy_gradient_loss  | -0.0106       |
|    value_loss            | 0.173         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 184           |
|    ep_rew_mean           | 5.62          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 112           |
|    time_elapsed          | 8701          |
|    total_timesteps       | 917504        |
| train/                   |               |
|    approx_kl             | 0.0068045016  |
|    clip_fraction         | 0.291         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.22         |
|    explained_variance    | 0.849         |
|    icm_loss              | 4.44e-10      |
|    intrinsic_reward_mean | 4.4448162e-10 |
|    intrinsic_reward_std  | 1.6655062e-10 |
|    learning_rate         | 0.000136      |
|    loss                  | -0.00421      |
|    n_updates             | 444           |
|    policy_gradient_loss  | -0.0118       |
|    value_loss            | 0.135         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 185           |
|    ep_rew_mean           | 5.63          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 113           |
|    time_elapsed          | 8776          |
|    total_timesteps       | 925696        |
| train/                   |               |
|    approx_kl             | 0.006364128   |
|    clip_fraction         | 0.291         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.17         |
|    explained_variance    | 0.865         |
|    icm_loss              | 4.48e-10      |
|    intrinsic_reward_mean | 4.475681e-10  |
|    intrinsic_reward_std  | 1.6495404e-10 |
|    learning_rate         | 0.000135      |
|    loss                  | 0.00672       |
|    n_updates             | 448           |
|    policy_gradient_loss  | -0.0126       |
|    value_loss            | 0.14          |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 188           |
|    ep_rew_mean           | 5.67          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 114           |
|    time_elapsed          | 8846          |
|    total_timesteps       | 933888        |
| train/                   |               |
|    approx_kl             | 0.006357435   |
|    clip_fraction         | 0.281         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.17         |
|    explained_variance    | 0.895         |
|    icm_loss              | 4.47e-10      |
|    intrinsic_reward_mean | 4.4746806e-10 |
|    intrinsic_reward_std  | 1.8502383e-10 |
|    learning_rate         | 0.000134      |
|    loss                  | 0.0148        |
|    n_updates             | 452           |
|    policy_gradient_loss  | -0.0114       |
|    value_loss            | 0.131         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 188           |
|    ep_rew_mean           | 5.62          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 115           |
|    time_elapsed          | 8922          |
|    total_timesteps       | 942080        |
| train/                   |               |
|    approx_kl             | 0.0066904253  |
|    clip_fraction         | 0.308         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.17         |
|    explained_variance    | 0.865         |
|    icm_loss              | 4.43e-10      |
|    intrinsic_reward_mean | 4.4345125e-10 |
|    intrinsic_reward_std  | 1.7327711e-10 |
|    learning_rate         | 0.000133      |
|    loss                  | 0.000426      |
|    n_updates             | 456           |
|    policy_gradient_loss  | -0.0104       |
|    value_loss            | 0.138         |
--------------------------------------------
Eval num_timesteps=950000, episode_reward=5.30 +/- 1.78
Episode length: 173.70 +/- 44.29
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 174           |
|    mean_reward           | 5.3           |
| time/                    |               |
|    total_timesteps       | 950000        |
| train/                   |               |
|    approx_kl             | 0.006757104   |
|    clip_fraction         | 0.281         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.15         |
|    explained_variance    | 0.875         |
|    icm_loss              | 4.42e-10      |
|    intrinsic_reward_mean | 4.4239795e-10 |
|    intrinsic_reward_std  | 1.9367986e-10 |
|    learning_rate         | 0.000132      |
|    loss                  | 0.0166        |
|    n_updates             | 460           |
|    policy_gradient_loss  | -0.00989      |
|    value_loss            | 0.159         |
--------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 5.66     |
| time/              |          |
|    fps             | 105      |
|    iterations      | 116      |
|    time_elapsed    | 9010     |
|    total_timesteps | 950272   |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 181           |
|    ep_rew_mean           | 5.52          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 117           |
|    time_elapsed          | 9087          |
|    total_timesteps       | 958464        |
| train/                   |               |
|    approx_kl             | 0.0066189393  |
|    clip_fraction         | 0.292         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.15         |
|    explained_variance    | 0.881         |
|    icm_loss              | 4.35e-10      |
|    intrinsic_reward_mean | 4.3533574e-10 |
|    intrinsic_reward_std  | 1.5240262e-10 |
|    learning_rate         | 0.000131      |
|    loss                  | -0.00857      |
|    n_updates             | 464           |
|    policy_gradient_loss  | -0.0119       |
|    value_loss            | 0.135         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 173           |
|    ep_rew_mean           | 5.28          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 118           |
|    time_elapsed          | 9163          |
|    total_timesteps       | 966656        |
| train/                   |               |
|    approx_kl             | 0.006846993   |
|    clip_fraction         | 0.284         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.1          |
|    explained_variance    | 0.871         |
|    icm_loss              | 4.33e-10      |
|    intrinsic_reward_mean | 4.3285042e-10 |
|    intrinsic_reward_std  | 1.7332617e-10 |
|    learning_rate         | 0.00013       |
|    loss                  | 0.012         |
|    n_updates             | 468           |
|    policy_gradient_loss  | -0.00939      |
|    value_loss            | 0.159         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 174           |
|    ep_rew_mean           | 5.34          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 119           |
|    time_elapsed          | 9237          |
|    total_timesteps       | 974848        |
| train/                   |               |
|    approx_kl             | 0.006212309   |
|    clip_fraction         | 0.282         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.11         |
|    explained_variance    | 0.859         |
|    icm_loss              | 4.33e-10      |
|    intrinsic_reward_mean | 4.3302983e-10 |
|    intrinsic_reward_std  | 1.7657839e-10 |
|    learning_rate         | 0.000129      |
|    loss                  | 0.0337        |
|    n_updates             | 472           |
|    policy_gradient_loss  | -0.0118       |
|    value_loss            | 0.161         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 177           |
|    ep_rew_mean           | 5.36          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 120           |
|    time_elapsed          | 9316          |
|    total_timesteps       | 983040        |
| train/                   |               |
|    approx_kl             | 0.0062967464  |
|    clip_fraction         | 0.277         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.12         |
|    explained_variance    | 0.887         |
|    icm_loss              | 4.21e-10      |
|    intrinsic_reward_mean | 4.2081094e-10 |
|    intrinsic_reward_std  | 1.7866679e-10 |
|    learning_rate         | 0.000128      |
|    loss                  | 0.012         |
|    n_updates             | 476           |
|    policy_gradient_loss  | -0.012        |
|    value_loss            | 0.126         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 173           |
|    ep_rew_mean           | 5.52          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 121           |
|    time_elapsed          | 9391          |
|    total_timesteps       | 991232        |
| train/                   |               |
|    approx_kl             | 0.0068430267  |
|    clip_fraction         | 0.285         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.11         |
|    explained_variance    | 0.859         |
|    icm_loss              | 4.29e-10      |
|    intrinsic_reward_mean | 4.2870957e-10 |
|    intrinsic_reward_std  | 1.9387608e-10 |
|    learning_rate         | 0.000127      |
|    loss                  | 0.0255        |
|    n_updates             | 480           |
|    policy_gradient_loss  | -0.0105       |
|    value_loss            | 0.176         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 178           |
|    ep_rew_mean           | 5.91          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 122           |
|    time_elapsed          | 9466          |
|    total_timesteps       | 999424        |
| train/                   |               |
|    approx_kl             | 0.0067634033  |
|    clip_fraction         | 0.274         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.04         |
|    explained_variance    | 0.883         |
|    icm_loss              | 4.23e-10      |
|    intrinsic_reward_mean | 4.2257134e-10 |
|    intrinsic_reward_std  | 1.9907564e-10 |
|    learning_rate         | 0.000126      |
|    loss                  | 0.0205        |
|    n_updates             | 484           |
|    policy_gradient_loss  | -0.0113       |
|    value_loss            | 0.155         |
--------------------------------------------
Eval num_timesteps=1000000, episode_reward=3.30 +/- 1.25
Episode length: 167.40 +/- 48.42
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 167           |
|    mean_reward           | 3.3           |
| time/                    |               |
|    total_timesteps       | 1000000       |
| train/                   |               |
|    approx_kl             | 0.0068488447  |
|    clip_fraction         | 0.289         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.09         |
|    explained_variance    | 0.871         |
|    icm_loss              | 4.25e-10      |
|    intrinsic_reward_mean | 4.2489312e-10 |
|    intrinsic_reward_std  | 3.017494e-10  |
|    learning_rate         | 0.000125      |
|    loss                  | 0.0369        |
|    n_updates             | 488           |
|    policy_gradient_loss  | -0.0117       |
|    value_loss            | 0.16          |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 5.97     |
| time/              |          |
|    fps             | 105      |
|    iterations      | 123      |
|    time_elapsed    | 9557     |
|    total_timesteps | 1007616  |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 176           |
|    ep_rew_mean           | 5.85          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 124           |
|    time_elapsed          | 9632          |
|    total_timesteps       | 1015808       |
| train/                   |               |
|    approx_kl             | 0.0060436307  |
|    clip_fraction         | 0.263         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.04         |
|    explained_variance    | 0.861         |
|    icm_loss              | 4.06e-10      |
|    intrinsic_reward_mean | 4.0616654e-10 |
|    intrinsic_reward_std  | 1.913267e-10  |
|    learning_rate         | 0.000124      |
|    loss                  | 0.0172        |
|    n_updates             | 492           |
|    policy_gradient_loss  | -0.0119       |
|    value_loss            | 0.15          |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 179           |
|    ep_rew_mean           | 5.72          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 125           |
|    time_elapsed          | 9708          |
|    total_timesteps       | 1024000       |
| train/                   |               |
|    approx_kl             | 0.006749085   |
|    clip_fraction         | 0.284         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.07         |
|    explained_variance    | 0.886         |
|    icm_loss              | 4.13e-10      |
|    intrinsic_reward_mean | 4.133739e-10  |
|    intrinsic_reward_std  | 2.3180304e-10 |
|    learning_rate         | 0.000123      |
|    loss                  | 0.00272       |
|    n_updates             | 496           |
|    policy_gradient_loss  | -0.013        |
|    value_loss            | 0.135         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 177           |
|    ep_rew_mean           | 5.59          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 126           |
|    time_elapsed          | 9782          |
|    total_timesteps       | 1032192       |
| train/                   |               |
|    approx_kl             | 0.0063692317  |
|    clip_fraction         | 0.289         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.08         |
|    explained_variance    | 0.876         |
|    icm_loss              | 4.05e-10      |
|    intrinsic_reward_mean | 4.0503778e-10 |
|    intrinsic_reward_std  | 1.6760968e-10 |
|    learning_rate         | 0.000122      |
|    loss                  | 0.021         |
|    n_updates             | 500           |
|    policy_gradient_loss  | -0.0115       |
|    value_loss            | 0.137         |
--------------------------------------------
-------------------------------------------
| rollout/                 |              |
|    ep_len_mean           | 186          |
|    ep_rew_mean           | 5.92         |
| time/                    |              |
|    fps                   | 105          |
|    iterations            | 127          |
|    time_elapsed          | 9856         |
|    total_timesteps       | 1040384      |
| train/                   |              |
|    approx_kl             | 0.005822192  |
|    clip_fraction         | 0.269        |
|    clip_range            | 0.1          |
|    entropy_loss          | -2           |
|    explained_variance    | 0.891        |
|    icm_loss              | 4.03e-10     |
|    intrinsic_reward_mean | 4.029183e-10 |
|    intrinsic_reward_std  | 1.904123e-10 |
|    learning_rate         | 0.000121     |
|    loss                  | 0.00199      |
|    n_updates             | 504          |
|    policy_gradient_loss  | -0.0118      |
|    value_loss            | 0.126        |
-------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 188           |
|    ep_rew_mean           | 5.9           |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 128           |
|    time_elapsed          | 9927          |
|    total_timesteps       | 1048576       |
| train/                   |               |
|    approx_kl             | 0.006528018   |
|    clip_fraction         | 0.275         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.06         |
|    explained_variance    | 0.884         |
|    icm_loss              | 4.03e-10      |
|    intrinsic_reward_mean | 4.02898e-10   |
|    intrinsic_reward_std  | 1.9893755e-10 |
|    learning_rate         | 0.00012       |
|    loss                  | 0.0153        |
|    n_updates             | 508           |
|    policy_gradient_loss  | -0.0121       |
|    value_loss            | 0.131         |
--------------------------------------------
Eval num_timesteps=1050000, episode_reward=3.80 +/- 1.35
Episode length: 167.70 +/- 55.17
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 168           |
|    mean_reward           | 3.8           |
| time/                    |               |
|    total_timesteps       | 1050000       |
| train/                   |               |
|    approx_kl             | 0.0056871437  |
|    clip_fraction         | 0.257         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.04         |
|    explained_variance    | 0.863         |
|    icm_loss              | 4.02e-10      |
|    intrinsic_reward_mean | 4.0234605e-10 |
|    intrinsic_reward_std  | 1.5396966e-10 |
|    learning_rate         | 0.000119      |
|    loss                  | 0.0477        |
|    n_updates             | 512           |
|    policy_gradient_loss  | -0.00985      |
|    value_loss            | 0.156         |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 6.08     |
| time/              |          |
|    fps             | 105      |
|    iterations      | 129      |
|    time_elapsed    | 10018    |
|    total_timesteps | 1056768  |
---------------------------------
-------------------------------------------
| rollout/                 |              |
|    ep_len_mean           | 181          |
|    ep_rew_mean           | 5.82         |
| time/                    |              |
|    fps                   | 105          |
|    iterations            | 130          |
|    time_elapsed          | 10090        |
|    total_timesteps       | 1064960      |
| train/                   |              |
|    approx_kl             | 0.0064658197 |
|    clip_fraction         | 0.262        |
|    clip_range            | 0.1          |
|    entropy_loss          | -1.98        |
|    explained_variance    | 0.877        |
|    icm_loss              | 3.97e-10     |
|    intrinsic_reward_mean | 3.972952e-10 |
|    intrinsic_reward_std  | 1.689377e-10 |
|    learning_rate         | 0.000118     |
|    loss                  | 0.000818     |
|    n_updates             | 516          |
|    policy_gradient_loss  | -0.0107      |
|    value_loss            | 0.135        |
-------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 183           |
|    ep_rew_mean           | 5.72          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 131           |
|    time_elapsed          | 10164         |
|    total_timesteps       | 1073152       |
| train/                   |               |
|    approx_kl             | 0.00705402    |
|    clip_fraction         | 0.3           |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.02         |
|    explained_variance    | 0.888         |
|    icm_loss              | 3.98e-10      |
|    intrinsic_reward_mean | 3.981032e-10  |
|    intrinsic_reward_std  | 1.7449382e-10 |
|    learning_rate         | 0.000117      |
|    loss                  | 0.0057        |
|    n_updates             | 520           |
|    policy_gradient_loss  | -0.00976      |
|    value_loss            | 0.136         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 182           |
|    ep_rew_mean           | 5.88          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 132           |
|    time_elapsed          | 10238         |
|    total_timesteps       | 1081344       |
| train/                   |               |
|    approx_kl             | 0.0062617967  |
|    clip_fraction         | 0.286         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2.01         |
|    explained_variance    | 0.915         |
|    icm_loss              | 3.97e-10      |
|    intrinsic_reward_mean | 3.9678516e-10 |
|    intrinsic_reward_std  | 1.7666064e-10 |
|    learning_rate         | 0.000116      |
|    loss                  | -0.00454      |
|    n_updates             | 524           |
|    policy_gradient_loss  | -0.0117       |
|    value_loss            | 0.107         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 183           |
|    ep_rew_mean           | 5.77          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 133           |
|    time_elapsed          | 10312         |
|    total_timesteps       | 1089536       |
| train/                   |               |
|    approx_kl             | 0.0064255856  |
|    clip_fraction         | 0.253         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2            |
|    explained_variance    | 0.9           |
|    icm_loss              | 3.93e-10      |
|    intrinsic_reward_mean | 3.9327952e-10 |
|    intrinsic_reward_std  | 1.5813632e-10 |
|    learning_rate         | 0.000115      |
|    loss                  | 0.00917       |
|    n_updates             | 528           |
|    policy_gradient_loss  | -0.0117       |
|    value_loss            | 0.132         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 181           |
|    ep_rew_mean           | 6.08          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 134           |
|    time_elapsed          | 10389         |
|    total_timesteps       | 1097728       |
| train/                   |               |
|    approx_kl             | 0.005866171   |
|    clip_fraction         | 0.258         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.97         |
|    explained_variance    | 0.909         |
|    icm_loss              | 3.95e-10      |
|    intrinsic_reward_mean | 3.9523307e-10 |
|    intrinsic_reward_std  | 1.7909257e-10 |
|    learning_rate         | 0.000114      |
|    loss                  | -0.00315      |
|    n_updates             | 532           |
|    policy_gradient_loss  | -0.0115       |
|    value_loss            | 0.125         |
--------------------------------------------
Eval num_timesteps=1100000, episode_reward=4.00 +/- 1.51
Episode length: 161.30 +/- 69.03
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 161           |
|    mean_reward           | 4             |
| time/                    |               |
|    total_timesteps       | 1100000       |
| train/                   |               |
|    approx_kl             | 0.0058874795  |
|    clip_fraction         | 0.264         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.96         |
|    explained_variance    | 0.911         |
|    icm_loss              | 3.9e-10       |
|    intrinsic_reward_mean | 3.9002676e-10 |
|    intrinsic_reward_std  | 1.7947449e-10 |
|    learning_rate         | 0.000113      |
|    loss                  | -0.00257      |
|    n_updates             | 536           |
|    policy_gradient_loss  | -0.0122       |
|    value_loss            | 0.119         |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 176      |
|    ep_rew_mean     | 6.2      |
| time/              |          |
|    fps             | 105      |
|    iterations      | 135      |
|    time_elapsed    | 10479    |
|    total_timesteps | 1105920  |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 182           |
|    ep_rew_mean           | 6.03          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 136           |
|    time_elapsed          | 10552         |
|    total_timesteps       | 1114112       |
| train/                   |               |
|    approx_kl             | 0.0060674394  |
|    clip_fraction         | 0.28          |
|    clip_range            | 0.1           |
|    entropy_loss          | -2            |
|    explained_variance    | 0.899         |
|    icm_loss              | 3.9e-10       |
|    intrinsic_reward_mean | 3.901779e-10  |
|    intrinsic_reward_std  | 1.4601881e-10 |
|    learning_rate         | 0.000112      |
|    loss                  | -0.007        |
|    n_updates             | 540           |
|    policy_gradient_loss  | -0.0112       |
|    value_loss            | 0.13          |
--------------------------------------------
-------------------------------------------
| rollout/                 |              |
|    ep_len_mean           | 184          |
|    ep_rew_mean           | 5.92         |
| time/                    |              |
|    fps                   | 105          |
|    iterations            | 137          |
|    time_elapsed          | 10624        |
|    total_timesteps       | 1122304      |
| train/                   |              |
|    approx_kl             | 0.0061714975 |
|    clip_fraction         | 0.274        |
|    clip_range            | 0.1          |
|    entropy_loss          | -2.03        |
|    explained_variance    | 0.9          |
|    icm_loss              | 3.91e-10     |
|    intrinsic_reward_mean | 3.912154e-10 |
|    intrinsic_reward_std  | 1.639056e-10 |
|    learning_rate         | 0.000111     |
|    loss                  | -0.00896     |
|    n_updates             | 544          |
|    policy_gradient_loss  | -0.0105      |
|    value_loss            | 0.127        |
-------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 186           |
|    ep_rew_mean           | 6.12          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 138           |
|    time_elapsed          | 10698         |
|    total_timesteps       | 1130496       |
| train/                   |               |
|    approx_kl             | 0.0062274686  |
|    clip_fraction         | 0.277         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.99         |
|    explained_variance    | 0.88          |
|    icm_loss              | 3.87e-10      |
|    intrinsic_reward_mean | 3.8740236e-10 |
|    intrinsic_reward_std  | 1.4324882e-10 |
|    learning_rate         | 0.00011       |
|    loss                  | 0.0256        |
|    n_updates             | 548           |
|    policy_gradient_loss  | -0.0105       |
|    value_loss            | 0.146         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 178           |
|    ep_rew_mean           | 6.05          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 139           |
|    time_elapsed          | 10777         |
|    total_timesteps       | 1138688       |
| train/                   |               |
|    approx_kl             | 0.006278064   |
|    clip_fraction         | 0.269         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.96         |
|    explained_variance    | 0.898         |
|    icm_loss              | 3.92e-10      |
|    intrinsic_reward_mean | 3.9165157e-10 |
|    intrinsic_reward_std  | 1.6213723e-10 |
|    learning_rate         | 0.000109      |
|    loss                  | 0.0101        |
|    n_updates             | 552           |
|    policy_gradient_loss  | -0.0119       |
|    value_loss            | 0.13          |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 168           |
|    ep_rew_mean           | 5.73          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 140           |
|    time_elapsed          | 10856         |
|    total_timesteps       | 1146880       |
| train/                   |               |
|    approx_kl             | 0.0061760554  |
|    clip_fraction         | 0.27          |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.97         |
|    explained_variance    | 0.895         |
|    icm_loss              | 3.84e-10      |
|    intrinsic_reward_mean | 3.843685e-10  |
|    intrinsic_reward_std  | 1.4906013e-10 |
|    learning_rate         | 0.000108      |
|    loss                  | 0.00915       |
|    n_updates             | 556           |
|    policy_gradient_loss  | -0.01         |
|    value_loss            | 0.136         |
--------------------------------------------
Eval num_timesteps=1150000, episode_reward=3.80 +/- 1.55
Episode length: 185.10 +/- 52.35
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 185           |
|    mean_reward           | 3.8           |
| time/                    |               |
|    total_timesteps       | 1150000       |
| train/                   |               |
|    approx_kl             | 0.005374766   |
|    clip_fraction         | 0.262         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.96         |
|    explained_variance    | 0.873         |
|    icm_loss              | 3.81e-10      |
|    intrinsic_reward_mean | 3.8105957e-10 |
|    intrinsic_reward_std  | 1.5080628e-10 |
|    learning_rate         | 0.000107      |
|    loss                  | 0.0249        |
|    n_updates             | 560           |
|    policy_gradient_loss  | -0.0104       |
|    value_loss            | 0.161         |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 176      |
|    ep_rew_mean     | 5.89     |
| time/              |          |
|    fps             | 105      |
|    iterations      | 141      |
|    time_elapsed    | 10943    |
|    total_timesteps | 1155072  |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 179           |
|    ep_rew_mean           | 5.81          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 142           |
|    time_elapsed          | 11018         |
|    total_timesteps       | 1163264       |
| train/                   |               |
|    approx_kl             | 0.0060717436  |
|    clip_fraction         | 0.257         |
|    clip_range            | 0.1           |
|    entropy_loss          | -2            |
|    explained_variance    | 0.879         |
|    icm_loss              | 3.79e-10      |
|    intrinsic_reward_mean | 3.7863152e-10 |
|    intrinsic_reward_std  | 1.5119e-10    |
|    learning_rate         | 0.000106      |
|    loss                  | 0.0244        |
|    n_updates             | 564           |
|    policy_gradient_loss  | -0.0118       |
|    value_loss            | 0.147         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 186           |
|    ep_rew_mean           | 6.21          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 143           |
|    time_elapsed          | 11091         |
|    total_timesteps       | 1171456       |
| train/                   |               |
|    approx_kl             | 0.00623467    |
|    clip_fraction         | 0.26          |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.95         |
|    explained_variance    | 0.898         |
|    icm_loss              | 3.78e-10      |
|    intrinsic_reward_mean | 3.7844244e-10 |
|    intrinsic_reward_std  | 1.5295172e-10 |
|    learning_rate         | 0.000105      |
|    loss                  | 0.022         |
|    n_updates             | 568           |
|    policy_gradient_loss  | -0.00983      |
|    value_loss            | 0.138         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 190           |
|    ep_rew_mean           | 6.44          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 144           |
|    time_elapsed          | 11164         |
|    total_timesteps       | 1179648       |
| train/                   |               |
|    approx_kl             | 0.00563403    |
|    clip_fraction         | 0.257         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.95         |
|    explained_variance    | 0.867         |
|    icm_loss              | 3.73e-10      |
|    intrinsic_reward_mean | 3.733728e-10  |
|    intrinsic_reward_std  | 1.4770929e-10 |
|    learning_rate         | 0.000104      |
|    loss                  | 0.0314        |
|    n_updates             | 572           |
|    policy_gradient_loss  | -0.0119       |
|    value_loss            | 0.151         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 181           |
|    ep_rew_mean           | 6.37          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 145           |
|    time_elapsed          | 11242         |
|    total_timesteps       | 1187840       |
| train/                   |               |
|    approx_kl             | 0.0062072137  |
|    clip_fraction         | 0.259         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.92         |
|    explained_variance    | 0.882         |
|    icm_loss              | 3.75e-10      |
|    intrinsic_reward_mean | 3.746135e-10  |
|    intrinsic_reward_std  | 1.5296668e-10 |
|    learning_rate         | 0.000103      |
|    loss                  | 0.00899       |
|    n_updates             | 576           |
|    policy_gradient_loss  | -0.0118       |
|    value_loss            | 0.153         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 178           |
|    ep_rew_mean           | 6.29          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 146           |
|    time_elapsed          | 11314         |
|    total_timesteps       | 1196032       |
| train/                   |               |
|    approx_kl             | 0.006118186   |
|    clip_fraction         | 0.269         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.92         |
|    explained_variance    | 0.9           |
|    icm_loss              | 3.71e-10      |
|    intrinsic_reward_mean | 3.709321e-10  |
|    intrinsic_reward_std  | 1.6175503e-10 |
|    learning_rate         | 0.000102      |
|    loss                  | 0.000363      |
|    n_updates             | 580           |
|    policy_gradient_loss  | -0.0113       |
|    value_loss            | 0.133         |
--------------------------------------------
Eval num_timesteps=1200000, episode_reward=4.70 +/- 1.43
Episode length: 150.50 +/- 51.66
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 150           |
|    mean_reward           | 4.7           |
| time/                    |               |
|    total_timesteps       | 1200000       |
| train/                   |               |
|    approx_kl             | 0.006150059   |
|    clip_fraction         | 0.256         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.92         |
|    explained_variance    | 0.896         |
|    icm_loss              | 3.69e-10      |
|    intrinsic_reward_mean | 3.688999e-10  |
|    intrinsic_reward_std  | 1.4892589e-10 |
|    learning_rate         | 0.0001        |
|    loss                  | 0.0191        |
|    n_updates             | 584           |
|    policy_gradient_loss  | -0.0108       |
|    value_loss            | 0.142         |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 6.22     |
| time/              |          |
|    fps             | 105      |
|    iterations      | 147      |
|    time_elapsed    | 11406    |
|    total_timesteps | 1204224  |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 187           |
|    ep_rew_mean           | 6.46          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 148           |
|    time_elapsed          | 11479         |
|    total_timesteps       | 1212416       |
| train/                   |               |
|    approx_kl             | 0.0056640827  |
|    clip_fraction         | 0.248         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.88         |
|    explained_variance    | 0.903         |
|    icm_loss              | 3.64e-10      |
|    intrinsic_reward_mean | 3.6440007e-10 |
|    intrinsic_reward_std  | 1.5854017e-10 |
|    learning_rate         | 9.95e-05      |
|    loss                  | -0.00445      |
|    n_updates             | 588           |
|    policy_gradient_loss  | -0.0102       |
|    value_loss            | 0.14          |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 182           |
|    ep_rew_mean           | 6.45          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 149           |
|    time_elapsed          | 11556         |
|    total_timesteps       | 1220608       |
| train/                   |               |
|    approx_kl             | 0.004973842   |
|    clip_fraction         | 0.233         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.91         |
|    explained_variance    | 0.891         |
|    icm_loss              | 3.64e-10      |
|    intrinsic_reward_mean | 3.6360698e-10 |
|    intrinsic_reward_std  | 1.4456537e-10 |
|    learning_rate         | 9.84e-05      |
|    loss                  | 0.00708       |
|    n_updates             | 592           |
|    policy_gradient_loss  | -0.0109       |
|    value_loss            | 0.141         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 178           |
|    ep_rew_mean           | 6.07          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 150           |
|    time_elapsed          | 11630         |
|    total_timesteps       | 1228800       |
| train/                   |               |
|    approx_kl             | 0.0060425615  |
|    clip_fraction         | 0.258         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.85         |
|    explained_variance    | 0.904         |
|    icm_loss              | 3.6e-10       |
|    intrinsic_reward_mean | 3.5985503e-10 |
|    intrinsic_reward_std  | 1.4585382e-10 |
|    learning_rate         | 9.74e-05      |
|    loss                  | 0.00621       |
|    n_updates             | 596           |
|    policy_gradient_loss  | -0.0102       |
|    value_loss            | 0.134         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 178           |
|    ep_rew_mean           | 6.03          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 151           |
|    time_elapsed          | 11706         |
|    total_timesteps       | 1236992       |
| train/                   |               |
|    approx_kl             | 0.005778301   |
|    clip_fraction         | 0.239         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.96         |
|    explained_variance    | 0.89          |
|    icm_loss              | 3.61e-10      |
|    intrinsic_reward_mean | 3.614608e-10  |
|    intrinsic_reward_std  | 1.5305006e-10 |
|    learning_rate         | 9.64e-05      |
|    loss                  | 0.0107        |
|    n_updates             | 600           |
|    policy_gradient_loss  | -0.011        |
|    value_loss            | 0.146         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 181           |
|    ep_rew_mean           | 6.08          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 152           |
|    time_elapsed          | 11777         |
|    total_timesteps       | 1245184       |
| train/                   |               |
|    approx_kl             | 0.0055331257  |
|    clip_fraction         | 0.255         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.9          |
|    explained_variance    | 0.9           |
|    icm_loss              | 3.61e-10      |
|    intrinsic_reward_mean | 3.60801e-10   |
|    intrinsic_reward_std  | 1.5862335e-10 |
|    learning_rate         | 9.54e-05      |
|    loss                  | 0.0126        |
|    n_updates             | 604           |
|    policy_gradient_loss  | -0.0125       |
|    value_loss            | 0.15          |
--------------------------------------------
Eval num_timesteps=1250000, episode_reward=4.20 +/- 1.22
Episode length: 196.80 +/- 59.60
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 197           |
|    mean_reward           | 4.2           |
| time/                    |               |
|    total_timesteps       | 1250000       |
| train/                   |               |
|    approx_kl             | 0.0057563055  |
|    clip_fraction         | 0.252         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.91         |
|    explained_variance    | 0.91          |
|    icm_loss              | 3.57e-10      |
|    intrinsic_reward_mean | 3.565485e-10  |
|    intrinsic_reward_std  | 1.6388486e-10 |
|    learning_rate         | 9.44e-05      |
|    loss                  | 0.000432      |
|    n_updates             | 608           |
|    policy_gradient_loss  | -0.0106       |
|    value_loss            | 0.127         |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 176      |
|    ep_rew_mean     | 6.09     |
| time/              |          |
|    fps             | 105      |
|    iterations      | 153      |
|    time_elapsed    | 11869    |
|    total_timesteps | 1253376  |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 173           |
|    ep_rew_mean           | 6.08          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 154           |
|    time_elapsed          | 11943         |
|    total_timesteps       | 1261568       |
| train/                   |               |
|    approx_kl             | 0.0062294845  |
|    clip_fraction         | 0.254         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.92         |
|    explained_variance    | 0.889         |
|    icm_loss              | 3.6e-10       |
|    intrinsic_reward_mean | 3.604676e-10  |
|    intrinsic_reward_std  | 1.2845146e-10 |
|    learning_rate         | 9.33e-05      |
|    loss                  | 0.0115        |
|    n_updates             | 612           |
|    policy_gradient_loss  | -0.00984      |
|    value_loss            | 0.152         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 176           |
|    ep_rew_mean           | 6.11          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 155           |
|    time_elapsed          | 12018         |
|    total_timesteps       | 1269760       |
| train/                   |               |
|    approx_kl             | 0.0062682712  |
|    clip_fraction         | 0.259         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.88         |
|    explained_variance    | 0.87          |
|    icm_loss              | 3.54e-10      |
|    intrinsic_reward_mean | 3.537421e-10  |
|    intrinsic_reward_std  | 1.5654861e-10 |
|    learning_rate         | 9.23e-05      |
|    loss                  | 0.0254        |
|    n_updates             | 616           |
|    policy_gradient_loss  | -0.0111       |
|    value_loss            | 0.175         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 181           |
|    ep_rew_mean           | 6.22          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 156           |
|    time_elapsed          | 12094         |
|    total_timesteps       | 1277952       |
| train/                   |               |
|    approx_kl             | 0.0059390524  |
|    clip_fraction         | 0.246         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.87         |
|    explained_variance    | 0.893         |
|    icm_loss              | 3.53e-10      |
|    intrinsic_reward_mean | 3.534417e-10  |
|    intrinsic_reward_std  | 1.3576298e-10 |
|    learning_rate         | 9.13e-05      |
|    loss                  | 0.0102        |
|    n_updates             | 620           |
|    policy_gradient_loss  | -0.00963      |
|    value_loss            | 0.151         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 187           |
|    ep_rew_mean           | 6.51          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 157           |
|    time_elapsed          | 12167         |
|    total_timesteps       | 1286144       |
| train/                   |               |
|    approx_kl             | 0.006092229   |
|    clip_fraction         | 0.249         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.87         |
|    explained_variance    | 0.883         |
|    icm_loss              | 3.56e-10      |
|    intrinsic_reward_mean | 3.5603515e-10 |
|    intrinsic_reward_std  | 1.4715744e-10 |
|    learning_rate         | 9.03e-05      |
|    loss                  | 0.0123        |
|    n_updates             | 624           |
|    policy_gradient_loss  | -0.0115       |
|    value_loss            | 0.162         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 183           |
|    ep_rew_mean           | 6.55          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 158           |
|    time_elapsed          | 12244         |
|    total_timesteps       | 1294336       |
| train/                   |               |
|    approx_kl             | 0.005899565   |
|    clip_fraction         | 0.26          |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.89         |
|    explained_variance    | 0.905         |
|    icm_loss              | 3.52e-10      |
|    intrinsic_reward_mean | 3.5228767e-10 |
|    intrinsic_reward_std  | 1.4416403e-10 |
|    learning_rate         | 8.92e-05      |
|    loss                  | 0.0184        |
|    n_updates             | 628           |
|    policy_gradient_loss  | -0.0107       |
|    value_loss            | 0.142         |
--------------------------------------------
Eval num_timesteps=1300000, episode_reward=4.80 +/- 1.55
Episode length: 163.20 +/- 46.94
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 163           |
|    mean_reward           | 4.8           |
| time/                    |               |
|    total_timesteps       | 1300000       |
| train/                   |               |
|    approx_kl             | 0.005218388   |
|    clip_fraction         | 0.251         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.83         |
|    explained_variance    | 0.898         |
|    icm_loss              | 3.54e-10      |
|    intrinsic_reward_mean | 3.5418113e-10 |
|    intrinsic_reward_std  | 1.4615192e-10 |
|    learning_rate         | 8.82e-05      |
|    loss                  | 0.0413        |
|    n_updates             | 632           |
|    policy_gradient_loss  | -0.0101       |
|    value_loss            | 0.162         |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 6.56     |
| time/              |          |
|    fps             | 105      |
|    iterations      | 159      |
|    time_elapsed    | 12336    |
|    total_timesteps | 1302528  |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 180           |
|    ep_rew_mean           | 6.65          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 160           |
|    time_elapsed          | 12412         |
|    total_timesteps       | 1310720       |
| train/                   |               |
|    approx_kl             | 0.0058084563  |
|    clip_fraction         | 0.255         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.83         |
|    explained_variance    | 0.902         |
|    icm_loss              | 3.47e-10      |
|    intrinsic_reward_mean | 3.470111e-10  |
|    intrinsic_reward_std  | 1.5282237e-10 |
|    learning_rate         | 8.72e-05      |
|    loss                  | 0.0563        |
|    n_updates             | 636           |
|    policy_gradient_loss  | -0.0107       |
|    value_loss            | 0.16          |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 180           |
|    ep_rew_mean           | 6.68          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 161           |
|    time_elapsed          | 12485         |
|    total_timesteps       | 1318912       |
| train/                   |               |
|    approx_kl             | 0.0053238994  |
|    clip_fraction         | 0.235         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.86         |
|    explained_variance    | 0.884         |
|    icm_loss              | 3.47e-10      |
|    intrinsic_reward_mean | 3.4685982e-10 |
|    intrinsic_reward_std  | 1.5566498e-10 |
|    learning_rate         | 8.62e-05      |
|    loss                  | 0.0264        |
|    n_updates             | 640           |
|    policy_gradient_loss  | -0.00994      |
|    value_loss            | 0.165         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 191           |
|    ep_rew_mean           | 6.52          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 162           |
|    time_elapsed          | 12557         |
|    total_timesteps       | 1327104       |
| train/                   |               |
|    approx_kl             | 0.005616165   |
|    clip_fraction         | 0.237         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.82         |
|    explained_variance    | 0.892         |
|    icm_loss              | 3.43e-10      |
|    intrinsic_reward_mean | 3.433973e-10  |
|    intrinsic_reward_std  | 1.7359182e-10 |
|    learning_rate         | 8.51e-05      |
|    loss                  | 0.0225        |
|    n_updates             | 644           |
|    policy_gradient_loss  | -0.0111       |
|    value_loss            | 0.158         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 185           |
|    ep_rew_mean           | 6.26          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 163           |
|    time_elapsed          | 12629         |
|    total_timesteps       | 1335296       |
| train/                   |               |
|    approx_kl             | 0.0046038246  |
|    clip_fraction         | 0.219         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.87         |
|    explained_variance    | 0.9           |
|    icm_loss              | 3.44e-10      |
|    intrinsic_reward_mean | 3.4353287e-10 |
|    intrinsic_reward_std  | 1.5790572e-10 |
|    learning_rate         | 8.41e-05      |
|    loss                  | 0.0281        |
|    n_updates             | 648           |
|    policy_gradient_loss  | -0.0101       |
|    value_loss            | 0.145         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 188           |
|    ep_rew_mean           | 6.42          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 164           |
|    time_elapsed          | 12700         |
|    total_timesteps       | 1343488       |
| train/                   |               |
|    approx_kl             | 0.0050911475  |
|    clip_fraction         | 0.228         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.85         |
|    explained_variance    | 0.905         |
|    icm_loss              | 3.45e-10      |
|    intrinsic_reward_mean | 3.450582e-10  |
|    intrinsic_reward_std  | 1.3774487e-10 |
|    learning_rate         | 8.31e-05      |
|    loss                  | 0.0283        |
|    n_updates             | 652           |
|    policy_gradient_loss  | -0.011        |
|    value_loss            | 0.153         |
--------------------------------------------
Eval num_timesteps=1350000, episode_reward=4.50 +/- 1.02
Episode length: 166.10 +/- 37.49
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 166           |
|    mean_reward           | 4.5           |
| time/                    |               |
|    total_timesteps       | 1350000       |
| train/                   |               |
|    approx_kl             | 0.0051947823  |
|    clip_fraction         | 0.222         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.89         |
|    explained_variance    | 0.902         |
|    icm_loss              | 3.43e-10      |
|    intrinsic_reward_mean | 3.429946e-10  |
|    intrinsic_reward_std  | 1.3556654e-10 |
|    learning_rate         | 8.21e-05      |
|    loss                  | 0.00843       |
|    n_updates             | 656           |
|    policy_gradient_loss  | -0.0125       |
|    value_loss            | 0.141         |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 6.32     |
| time/              |          |
|    fps             | 105      |
|    iterations      | 165      |
|    time_elapsed    | 12790    |
|    total_timesteps | 1351680  |
---------------------------------
-------------------------------------------
| rollout/                 |              |
|    ep_len_mean           | 181          |
|    ep_rew_mean           | 6.29         |
| time/                    |              |
|    fps                   | 105          |
|    iterations            | 166          |
|    time_elapsed          | 12866        |
|    total_timesteps       | 1359872      |
| train/                   |              |
|    approx_kl             | 0.005483961  |
|    clip_fraction         | 0.213        |
|    clip_range            | 0.1          |
|    entropy_loss          | -1.85        |
|    explained_variance    | 0.894        |
|    icm_loss              | 3.41e-10     |
|    intrinsic_reward_mean | 3.410816e-10 |
|    intrinsic_reward_std  | 1.404362e-10 |
|    learning_rate         | 8.1e-05      |
|    loss                  | 0.0133       |
|    n_updates             | 660          |
|    policy_gradient_loss  | -0.0118      |
|    value_loss            | 0.154        |
-------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 179           |
|    ep_rew_mean           | 6.46          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 167           |
|    time_elapsed          | 12941         |
|    total_timesteps       | 1368064       |
| train/                   |               |
|    approx_kl             | 0.004769026   |
|    clip_fraction         | 0.22          |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.83         |
|    explained_variance    | 0.893         |
|    icm_loss              | 3.38e-10      |
|    intrinsic_reward_mean | 3.3783648e-10 |
|    intrinsic_reward_std  | 1.3224043e-10 |
|    learning_rate         | 8e-05         |
|    loss                  | 0.0222        |
|    n_updates             | 664           |
|    policy_gradient_loss  | -0.0104       |
|    value_loss            | 0.168         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 176           |
|    ep_rew_mean           | 6.3           |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 168           |
|    time_elapsed          | 13019         |
|    total_timesteps       | 1376256       |
| train/                   |               |
|    approx_kl             | 0.0049842745  |
|    clip_fraction         | 0.231         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.81         |
|    explained_variance    | 0.898         |
|    icm_loss              | 3.39e-10      |
|    intrinsic_reward_mean | 3.3948208e-10 |
|    intrinsic_reward_std  | 1.2966204e-10 |
|    learning_rate         | 7.9e-05       |
|    loss                  | 0.0272        |
|    n_updates             | 668           |
|    policy_gradient_loss  | -0.00984      |
|    value_loss            | 0.156         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 181           |
|    ep_rew_mean           | 6.6           |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 169           |
|    time_elapsed          | 13088         |
|    total_timesteps       | 1384448       |
| train/                   |               |
|    approx_kl             | 0.0048789843  |
|    clip_fraction         | 0.219         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.82         |
|    explained_variance    | 0.886         |
|    icm_loss              | 3.39e-10      |
|    intrinsic_reward_mean | 3.3858322e-10 |
|    intrinsic_reward_std  | 1.4476194e-10 |
|    learning_rate         | 7.8e-05       |
|    loss                  | 0.0393        |
|    n_updates             | 672           |
|    policy_gradient_loss  | -0.00969      |
|    value_loss            | 0.184         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 188           |
|    ep_rew_mean           | 6.55          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 170           |
|    time_elapsed          | 13162         |
|    total_timesteps       | 1392640       |
| train/                   |               |
|    approx_kl             | 0.005193189   |
|    clip_fraction         | 0.218         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.83         |
|    explained_variance    | 0.889         |
|    icm_loss              | 3.41e-10      |
|    intrinsic_reward_mean | 3.4065145e-10 |
|    intrinsic_reward_std  | 1.5468617e-10 |
|    learning_rate         | 7.69e-05      |
|    loss                  | 0.0174        |
|    n_updates             | 676           |
|    policy_gradient_loss  | -0.00954      |
|    value_loss            | 0.16          |
--------------------------------------------
Eval num_timesteps=1400000, episode_reward=5.40 +/- 1.68
Episode length: 170.20 +/- 67.43
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 170           |
|    mean_reward           | 5.4           |
| time/                    |               |
|    total_timesteps       | 1400000       |
| train/                   |               |
|    approx_kl             | 0.0048954627  |
|    clip_fraction         | 0.207         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.81         |
|    explained_variance    | 0.9           |
|    icm_loss              | 3.34e-10      |
|    intrinsic_reward_mean | 3.3402098e-10 |
|    intrinsic_reward_std  | 1.5384845e-10 |
|    learning_rate         | 7.59e-05      |
|    loss                  | 0.0232        |
|    n_updates             | 680           |
|    policy_gradient_loss  | -0.011        |
|    value_loss            | 0.15          |
--------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 6.67     |
| time/              |          |
|    fps             | 105      |
|    iterations      | 171      |
|    time_elapsed    | 13249    |
|    total_timesteps | 1400832  |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 183           |
|    ep_rew_mean           | 6.73          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 172           |
|    time_elapsed          | 13321         |
|    total_timesteps       | 1409024       |
| train/                   |               |
|    approx_kl             | 0.004855058   |
|    clip_fraction         | 0.211         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.8          |
|    explained_variance    | 0.878         |
|    icm_loss              | 3.34e-10      |
|    intrinsic_reward_mean | 3.3439768e-10 |
|    intrinsic_reward_std  | 1.3713916e-10 |
|    learning_rate         | 7.49e-05      |
|    loss                  | 0.048         |
|    n_updates             | 684           |
|    policy_gradient_loss  | -0.00873      |
|    value_loss            | 0.197         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 178           |
|    ep_rew_mean           | 6.64          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 173           |
|    time_elapsed          | 13398         |
|    total_timesteps       | 1417216       |
| train/                   |               |
|    approx_kl             | 0.0048464546  |
|    clip_fraction         | 0.207         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.82         |
|    explained_variance    | 0.908         |
|    icm_loss              | 3.36e-10      |
|    intrinsic_reward_mean | 3.3608988e-10 |
|    intrinsic_reward_std  | 1.2312741e-10 |
|    learning_rate         | 7.39e-05      |
|    loss                  | 0.0221        |
|    n_updates             | 688           |
|    policy_gradient_loss  | -0.0117       |
|    value_loss            | 0.162         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 171           |
|    ep_rew_mean           | 6.56          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 174           |
|    time_elapsed          | 13476         |
|    total_timesteps       | 1425408       |
| train/                   |               |
|    approx_kl             | 0.0050072437  |
|    clip_fraction         | 0.205         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.81         |
|    explained_variance    | 0.887         |
|    icm_loss              | 3.33e-10      |
|    intrinsic_reward_mean | 3.3331501e-10 |
|    intrinsic_reward_std  | 1.4443322e-10 |
|    learning_rate         | 7.28e-05      |
|    loss                  | 0.0419        |
|    n_updates             | 692           |
|    policy_gradient_loss  | -0.0104       |
|    value_loss            | 0.176         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 173           |
|    ep_rew_mean           | 6.52          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 175           |
|    time_elapsed          | 13550         |
|    total_timesteps       | 1433600       |
| train/                   |               |
|    approx_kl             | 0.005041654   |
|    clip_fraction         | 0.213         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.79         |
|    explained_variance    | 0.885         |
|    icm_loss              | 3.26e-10      |
|    intrinsic_reward_mean | 3.2636957e-10 |
|    intrinsic_reward_std  | 1.1584984e-10 |
|    learning_rate         | 7.18e-05      |
|    loss                  | 0.0562        |
|    n_updates             | 696           |
|    policy_gradient_loss  | -0.00957      |
|    value_loss            | 0.196         |
--------------------------------------------
-------------------------------------------
| rollout/                 |              |
|    ep_len_mean           | 181          |
|    ep_rew_mean           | 6.47         |
| time/                    |              |
|    fps                   | 105          |
|    iterations            | 176          |
|    time_elapsed          | 13621        |
|    total_timesteps       | 1441792      |
| train/                   |              |
|    approx_kl             | 0.004781958  |
|    clip_fraction         | 0.211        |
|    clip_range            | 0.1          |
|    entropy_loss          | -1.79        |
|    explained_variance    | 0.9          |
|    icm_loss              | 3.31e-10     |
|    intrinsic_reward_mean | 3.313117e-10 |
|    intrinsic_reward_std  | 1.598338e-10 |
|    learning_rate         | 7.08e-05     |
|    loss                  | 0.0321       |
|    n_updates             | 700          |
|    policy_gradient_loss  | -0.0104      |
|    value_loss            | 0.175        |
-------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 185           |
|    ep_rew_mean           | 6.41          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 177           |
|    time_elapsed          | 13694         |
|    total_timesteps       | 1449984       |
| train/                   |               |
|    approx_kl             | 0.0048796344  |
|    clip_fraction         | 0.224         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.82         |
|    explained_variance    | 0.889         |
|    icm_loss              | 3.26e-10      |
|    intrinsic_reward_mean | 3.2633918e-10 |
|    intrinsic_reward_std  | 1.2074575e-10 |
|    learning_rate         | 6.98e-05      |
|    loss                  | 0.0307        |
|    n_updates             | 704           |
|    policy_gradient_loss  | -0.00975      |
|    value_loss            | 0.173         |
--------------------------------------------
Eval num_timesteps=1450000, episode_reward=4.60 +/- 1.20
Episode length: 156.00 +/- 73.69
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 156           |
|    mean_reward           | 4.6           |
| time/                    |               |
|    total_timesteps       | 1450000       |
| train/                   |               |
|    approx_kl             | 0.00454423    |
|    clip_fraction         | 0.194         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.88         |
|    explained_variance    | 0.873         |
|    icm_loss              | 3.3e-10       |
|    intrinsic_reward_mean | 3.302717e-10  |
|    intrinsic_reward_std  | 1.3599175e-10 |
|    learning_rate         | 6.88e-05      |
|    loss                  | 0.0199        |
|    n_updates             | 708           |
|    policy_gradient_loss  | -0.0107       |
|    value_loss            | 0.176         |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 6.43     |
| time/              |          |
|    fps             | 105      |
|    iterations      | 178      |
|    time_elapsed    | 13782    |
|    total_timesteps | 1458176  |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 185           |
|    ep_rew_mean           | 6.47          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 179           |
|    time_elapsed          | 13857         |
|    total_timesteps       | 1466368       |
| train/                   |               |
|    approx_kl             | 0.0046421424  |
|    clip_fraction         | 0.213         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.83         |
|    explained_variance    | 0.898         |
|    icm_loss              | 3.24e-10      |
|    intrinsic_reward_mean | 3.2407305e-10 |
|    intrinsic_reward_std  | 1.2222401e-10 |
|    learning_rate         | 6.77e-05      |
|    loss                  | 0.0323        |
|    n_updates             | 712           |
|    policy_gradient_loss  | -0.0101       |
|    value_loss            | 0.155         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 190           |
|    ep_rew_mean           | 6.81          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 180           |
|    time_elapsed          | 13927         |
|    total_timesteps       | 1474560       |
| train/                   |               |
|    approx_kl             | 0.0047540665  |
|    clip_fraction         | 0.215         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.77         |
|    explained_variance    | 0.91          |
|    icm_loss              | 3.22e-10      |
|    intrinsic_reward_mean | 3.2236835e-10 |
|    intrinsic_reward_std  | 1.2475228e-10 |
|    learning_rate         | 6.67e-05      |
|    loss                  | 0.00845       |
|    n_updates             | 716           |
|    policy_gradient_loss  | -0.0101       |
|    value_loss            | 0.141         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 186           |
|    ep_rew_mean           | 6.66          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 181           |
|    time_elapsed          | 14001         |
|    total_timesteps       | 1482752       |
| train/                   |               |
|    approx_kl             | 0.0044799233  |
|    clip_fraction         | 0.2           |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.74         |
|    explained_variance    | 0.915         |
|    icm_loss              | 3.25e-10      |
|    intrinsic_reward_mean | 3.2519132e-10 |
|    intrinsic_reward_std  | 1.2916583e-10 |
|    learning_rate         | 6.57e-05      |
|    loss                  | 0.0224        |
|    n_updates             | 720           |
|    policy_gradient_loss  | -0.00952      |
|    value_loss            | 0.154         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 178           |
|    ep_rew_mean           | 6.55          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 182           |
|    time_elapsed          | 14080         |
|    total_timesteps       | 1490944       |
| train/                   |               |
|    approx_kl             | 0.004288829   |
|    clip_fraction         | 0.194         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.77         |
|    explained_variance    | 0.907         |
|    icm_loss              | 3.21e-10      |
|    intrinsic_reward_mean | 3.2143824e-10 |
|    intrinsic_reward_std  | 1.3496489e-10 |
|    learning_rate         | 6.47e-05      |
|    loss                  | 0.0196        |
|    n_updates             | 724           |
|    policy_gradient_loss  | -0.0101       |
|    value_loss            | 0.165         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 171           |
|    ep_rew_mean           | 6.55          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 183           |
|    time_elapsed          | 14156         |
|    total_timesteps       | 1499136       |
| train/                   |               |
|    approx_kl             | 0.0043866243  |
|    clip_fraction         | 0.2           |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.75         |
|    explained_variance    | 0.884         |
|    icm_loss              | 3.21e-10      |
|    intrinsic_reward_mean | 3.2101116e-10 |
|    intrinsic_reward_std  | 1.2943542e-10 |
|    learning_rate         | 6.36e-05      |
|    loss                  | 0.059         |
|    n_updates             | 728           |
|    policy_gradient_loss  | -0.00883      |
|    value_loss            | 0.212         |
--------------------------------------------
Eval num_timesteps=1500000, episode_reward=5.00 +/- 2.02
Episode length: 178.70 +/- 62.92
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 179           |
|    mean_reward           | 5             |
| time/                    |               |
|    total_timesteps       | 1500000       |
| train/                   |               |
|    approx_kl             | 0.0043164734  |
|    clip_fraction         | 0.203         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.78         |
|    explained_variance    | 0.893         |
|    icm_loss              | 3.17e-10      |
|    intrinsic_reward_mean | 3.1718922e-10 |
|    intrinsic_reward_std  | 1.2965959e-10 |
|    learning_rate         | 6.26e-05      |
|    loss                  | 0.0393        |
|    n_updates             | 732           |
|    policy_gradient_loss  | -0.00999      |
|    value_loss            | 0.182         |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 6.76     |
| time/              |          |
|    fps             | 105      |
|    iterations      | 184      |
|    time_elapsed    | 14246    |
|    total_timesteps | 1507328  |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 185           |
|    ep_rew_mean           | 6.93          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 185           |
|    time_elapsed          | 14318         |
|    total_timesteps       | 1515520       |
| train/                   |               |
|    approx_kl             | 0.0040631974  |
|    clip_fraction         | 0.184         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.72         |
|    explained_variance    | 0.898         |
|    icm_loss              | 3.21e-10      |
|    intrinsic_reward_mean | 3.21353e-10   |
|    intrinsic_reward_std  | 1.1287487e-10 |
|    learning_rate         | 6.16e-05      |
|    loss                  | 0.0354        |
|    n_updates             | 736           |
|    policy_gradient_loss  | -0.00912      |
|    value_loss            | 0.177         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 190           |
|    ep_rew_mean           | 6.86          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 186           |
|    time_elapsed          | 14391         |
|    total_timesteps       | 1523712       |
| train/                   |               |
|    approx_kl             | 0.004660684   |
|    clip_fraction         | 0.208         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.74         |
|    explained_variance    | 0.885         |
|    icm_loss              | 3.16e-10      |
|    intrinsic_reward_mean | 3.1648356e-10 |
|    intrinsic_reward_std  | 1.3677617e-10 |
|    learning_rate         | 6.06e-05      |
|    loss                  | 0.0699        |
|    n_updates             | 740           |
|    policy_gradient_loss  | -0.00929      |
|    value_loss            | 0.191         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 191           |
|    ep_rew_mean           | 6.79          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 187           |
|    time_elapsed          | 14462         |
|    total_timesteps       | 1531904       |
| train/                   |               |
|    approx_kl             | 0.004659036   |
|    clip_fraction         | 0.203         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.74         |
|    explained_variance    | 0.896         |
|    icm_loss              | 3.19e-10      |
|    intrinsic_reward_mean | 3.1908476e-10 |
|    intrinsic_reward_std  | 1.2311764e-10 |
|    learning_rate         | 5.95e-05      |
|    loss                  | 0.0344        |
|    n_updates             | 744           |
|    policy_gradient_loss  | -0.00975      |
|    value_loss            | 0.17          |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 188           |
|    ep_rew_mean           | 6.8           |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 188           |
|    time_elapsed          | 14536         |
|    total_timesteps       | 1540096       |
| train/                   |               |
|    approx_kl             | 0.004586749   |
|    clip_fraction         | 0.203         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.79         |
|    explained_variance    | 0.908         |
|    icm_loss              | 3.16e-10      |
|    intrinsic_reward_mean | 3.1599579e-10 |
|    intrinsic_reward_std  | 1.0675467e-10 |
|    learning_rate         | 5.85e-05      |
|    loss                  | 0.0116        |
|    n_updates             | 748           |
|    policy_gradient_loss  | -0.0106       |
|    value_loss            | 0.14          |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 187           |
|    ep_rew_mean           | 6.65          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 189           |
|    time_elapsed          | 14606         |
|    total_timesteps       | 1548288       |
| train/                   |               |
|    approx_kl             | 0.0039201756  |
|    clip_fraction         | 0.18          |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.76         |
|    explained_variance    | 0.905         |
|    icm_loss              | 3.15e-10      |
|    intrinsic_reward_mean | 3.1531186e-10 |
|    intrinsic_reward_std  | 1.4341596e-10 |
|    learning_rate         | 5.75e-05      |
|    loss                  | 0.0322        |
|    n_updates             | 752           |
|    policy_gradient_loss  | -0.0104       |
|    value_loss            | 0.171         |
--------------------------------------------
Eval num_timesteps=1550000, episode_reward=5.30 +/- 1.78
Episode length: 181.70 +/- 41.61
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 182           |
|    mean_reward           | 5.3           |
| time/                    |               |
|    total_timesteps       | 1550000       |
| train/                   |               |
|    approx_kl             | 0.0045497864  |
|    clip_fraction         | 0.203         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.8          |
|    explained_variance    | 0.901         |
|    icm_loss              | 3.18e-10      |
|    intrinsic_reward_mean | 3.1830238e-10 |
|    intrinsic_reward_std  | 1.4515433e-10 |
|    learning_rate         | 5.65e-05      |
|    loss                  | 0.0297        |
|    n_updates             | 756           |
|    policy_gradient_loss  | -0.0105       |
|    value_loss            | 0.171         |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 6.58     |
| time/              |          |
|    fps             | 105      |
|    iterations      | 190      |
|    time_elapsed    | 14698    |
|    total_timesteps | 1556480  |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 179           |
|    ep_rew_mean           | 6.63          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 191           |
|    time_elapsed          | 14773         |
|    total_timesteps       | 1564672       |
| train/                   |               |
|    approx_kl             | 0.0042502447  |
|    clip_fraction         | 0.187         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.71         |
|    explained_variance    | 0.89          |
|    icm_loss              | 3.12e-10      |
|    intrinsic_reward_mean | 3.1167371e-10 |
|    intrinsic_reward_std  | 1.1933517e-10 |
|    learning_rate         | 5.54e-05      |
|    loss                  | 0.0739        |
|    n_updates             | 760           |
|    policy_gradient_loss  | -0.00956      |
|    value_loss            | 0.203         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 180           |
|    ep_rew_mean           | 6.54          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 192           |
|    time_elapsed          | 14849         |
|    total_timesteps       | 1572864       |
| train/                   |               |
|    approx_kl             | 0.0043630614  |
|    clip_fraction         | 0.182         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.68         |
|    explained_variance    | 0.907         |
|    icm_loss              | 3.14e-10      |
|    intrinsic_reward_mean | 3.1397512e-10 |
|    intrinsic_reward_std  | 1.2324539e-10 |
|    learning_rate         | 5.44e-05      |
|    loss                  | 0.0342        |
|    n_updates             | 764           |
|    policy_gradient_loss  | -0.00881      |
|    value_loss            | 0.169         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 188           |
|    ep_rew_mean           | 6.83          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 193           |
|    time_elapsed          | 14922         |
|    total_timesteps       | 1581056       |
| train/                   |               |
|    approx_kl             | 0.003763957   |
|    clip_fraction         | 0.162         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.71         |
|    explained_variance    | 0.902         |
|    icm_loss              | 3.17e-10      |
|    intrinsic_reward_mean | 3.174884e-10  |
|    intrinsic_reward_std  | 1.3085907e-10 |
|    learning_rate         | 5.34e-05      |
|    loss                  | 0.0555        |
|    n_updates             | 768           |
|    policy_gradient_loss  | -0.00883      |
|    value_loss            | 0.194         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 194           |
|    ep_rew_mean           | 6.98          |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 194           |
|    time_elapsed          | 14993         |
|    total_timesteps       | 1589248       |
| train/                   |               |
|    approx_kl             | 0.003971086   |
|    clip_fraction         | 0.177         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.72         |
|    explained_variance    | 0.916         |
|    icm_loss              | 3.12e-10      |
|    intrinsic_reward_mean | 3.115676e-10  |
|    intrinsic_reward_std  | 1.2137658e-10 |
|    learning_rate         | 5.24e-05      |
|    loss                  | 0.0232        |
|    n_updates             | 772           |
|    policy_gradient_loss  | -0.0101       |
|    value_loss            | 0.153         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 192           |
|    ep_rew_mean           | 7.07          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 195           |
|    time_elapsed          | 15064         |
|    total_timesteps       | 1597440       |
| train/                   |               |
|    approx_kl             | 0.0038460472  |
|    clip_fraction         | 0.173         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.69         |
|    explained_variance    | 0.91          |
|    icm_loss              | 3.14e-10      |
|    intrinsic_reward_mean | 3.140433e-10  |
|    intrinsic_reward_std  | 1.5174904e-10 |
|    learning_rate         | 5.13e-05      |
|    loss                  | 0.0414        |
|    n_updates             | 776           |
|    policy_gradient_loss  | -0.00979      |
|    value_loss            | 0.176         |
--------------------------------------------
Eval num_timesteps=1600000, episode_reward=5.60 +/- 1.36
Episode length: 178.10 +/- 30.94
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 178           |
|    mean_reward           | 5.6           |
| time/                    |               |
|    total_timesteps       | 1600000       |
| train/                   |               |
|    approx_kl             | 0.0035785446  |
|    clip_fraction         | 0.168         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.72         |
|    explained_variance    | 0.911         |
|    icm_loss              | 3.07e-10      |
|    intrinsic_reward_mean | 3.0684794e-10 |
|    intrinsic_reward_std  | 1.0860612e-10 |
|    learning_rate         | 5.03e-05      |
|    loss                  | 0.0407        |
|    n_updates             | 780           |
|    policy_gradient_loss  | -0.00879      |
|    value_loss            | 0.175         |
--------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 181      |
|    ep_rew_mean     | 6.74     |
| time/              |          |
|    fps             | 105      |
|    iterations      | 196      |
|    time_elapsed    | 15159    |
|    total_timesteps | 1605632  |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 181           |
|    ep_rew_mean           | 6.7           |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 197           |
|    time_elapsed          | 15229         |
|    total_timesteps       | 1613824       |
| train/                   |               |
|    approx_kl             | 0.0038849814  |
|    clip_fraction         | 0.168         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.68         |
|    explained_variance    | 0.9           |
|    icm_loss              | 3.09e-10      |
|    intrinsic_reward_mean | 3.091377e-10  |
|    intrinsic_reward_std  | 1.2212867e-10 |
|    learning_rate         | 4.93e-05      |
|    loss                  | 0.0436        |
|    n_updates             | 784           |
|    policy_gradient_loss  | -0.00901      |
|    value_loss            | 0.197         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 184           |
|    ep_rew_mean           | 6.92          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 198           |
|    time_elapsed          | 15299         |
|    total_timesteps       | 1622016       |
| train/                   |               |
|    approx_kl             | 0.0035582995  |
|    clip_fraction         | 0.167         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.7          |
|    explained_variance    | 0.911         |
|    icm_loss              | 3.11e-10      |
|    intrinsic_reward_mean | 3.106703e-10  |
|    intrinsic_reward_std  | 1.2097567e-10 |
|    learning_rate         | 4.83e-05      |
|    loss                  | 0.0401        |
|    n_updates             | 788           |
|    policy_gradient_loss  | -0.0076       |
|    value_loss            | 0.17          |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 180           |
|    ep_rew_mean           | 6.73          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 199           |
|    time_elapsed          | 15376         |
|    total_timesteps       | 1630208       |
| train/                   |               |
|    approx_kl             | 0.0036748317  |
|    clip_fraction         | 0.165         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.7          |
|    explained_variance    | 0.906         |
|    icm_loss              | 3.08e-10      |
|    intrinsic_reward_mean | 3.0818326e-10 |
|    intrinsic_reward_std  | 1.1507601e-10 |
|    learning_rate         | 4.72e-05      |
|    loss                  | 0.0322        |
|    n_updates             | 792           |
|    policy_gradient_loss  | -0.00986      |
|    value_loss            | 0.188         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 181           |
|    ep_rew_mean           | 6.71          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 200           |
|    time_elapsed          | 15447         |
|    total_timesteps       | 1638400       |
| train/                   |               |
|    approx_kl             | 0.0037402185  |
|    clip_fraction         | 0.176         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.72         |
|    explained_variance    | 0.9           |
|    icm_loss              | 3.11e-10      |
|    intrinsic_reward_mean | 3.1117242e-10 |
|    intrinsic_reward_std  | 1.210511e-10  |
|    learning_rate         | 4.62e-05      |
|    loss                  | 0.0526        |
|    n_updates             | 796           |
|    policy_gradient_loss  | -0.00848      |
|    value_loss            | 0.183         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 189           |
|    ep_rew_mean           | 6.86          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 201           |
|    time_elapsed          | 15522         |
|    total_timesteps       | 1646592       |
| train/                   |               |
|    approx_kl             | 0.0040119635  |
|    clip_fraction         | 0.183         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.74         |
|    explained_variance    | 0.887         |
|    icm_loss              | 3.04e-10      |
|    intrinsic_reward_mean | 3.0351222e-10 |
|    intrinsic_reward_std  | 1.2409214e-10 |
|    learning_rate         | 4.52e-05      |
|    loss                  | 0.0332        |
|    n_updates             | 800           |
|    policy_gradient_loss  | -0.00936      |
|    value_loss            | 0.186         |
--------------------------------------------
Eval num_timesteps=1650000, episode_reward=6.00 +/- 1.37
Episode length: 185.60 +/- 28.02
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 186           |
|    mean_reward           | 6             |
| time/                    |               |
|    total_timesteps       | 1650000       |
| train/                   |               |
|    approx_kl             | 0.003701236   |
|    clip_fraction         | 0.175         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.75         |
|    explained_variance    | 0.919         |
|    icm_loss              | 3.02e-10      |
|    intrinsic_reward_mean | 3.0244002e-10 |
|    intrinsic_reward_std  | 1.2362568e-10 |
|    learning_rate         | 4.42e-05      |
|    loss                  | 0.0317        |
|    n_updates             | 804           |
|    policy_gradient_loss  | -0.0102       |
|    value_loss            | 0.148         |
--------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 7.05     |
| time/              |          |
|    fps             | 105      |
|    iterations      | 202      |
|    time_elapsed    | 15612    |
|    total_timesteps | 1654784  |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 189           |
|    ep_rew_mean           | 7.14          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 203           |
|    time_elapsed          | 15685         |
|    total_timesteps       | 1662976       |
| train/                   |               |
|    approx_kl             | 0.0038192272  |
|    clip_fraction         | 0.156         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.71         |
|    explained_variance    | 0.89          |
|    icm_loss              | 3.07e-10      |
|    intrinsic_reward_mean | 3.0735645e-10 |
|    intrinsic_reward_std  | 1.3099424e-10 |
|    learning_rate         | 4.32e-05      |
|    loss                  | 0.0342        |
|    n_updates             | 808           |
|    policy_gradient_loss  | -0.00964      |
|    value_loss            | 0.207         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 187           |
|    ep_rew_mean           | 7.19          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 204           |
|    time_elapsed          | 15755         |
|    total_timesteps       | 1671168       |
| train/                   |               |
|    approx_kl             | 0.003075248   |
|    clip_fraction         | 0.148         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.72         |
|    explained_variance    | 0.891         |
|    icm_loss              | 3.03e-10      |
|    intrinsic_reward_mean | 3.029898e-10  |
|    intrinsic_reward_std  | 1.0626443e-10 |
|    learning_rate         | 4.21e-05      |
|    loss                  | 0.0417        |
|    n_updates             | 812           |
|    policy_gradient_loss  | -0.00887      |
|    value_loss            | 0.199         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 191           |
|    ep_rew_mean           | 6.88          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 205           |
|    time_elapsed          | 15827         |
|    total_timesteps       | 1679360       |
| train/                   |               |
|    approx_kl             | 0.0033795214  |
|    clip_fraction         | 0.15          |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.71         |
|    explained_variance    | 0.898         |
|    icm_loss              | 3.02e-10      |
|    intrinsic_reward_mean | 3.0178424e-10 |
|    intrinsic_reward_std  | 1.2132914e-10 |
|    learning_rate         | 4.11e-05      |
|    loss                  | 0.0509        |
|    n_updates             | 816           |
|    policy_gradient_loss  | -0.00893      |
|    value_loss            | 0.179         |
--------------------------------------------
---------------------------------------------
| rollout/                 |                |
|    ep_len_mean           | 194            |
|    ep_rew_mean           | 6.81           |
| time/                    |                |
|    fps                   | 106            |
|    iterations            | 206            |
|    time_elapsed          | 15896          |
|    total_timesteps       | 1687552        |
| train/                   |                |
|    approx_kl             | 0.0030137396   |
|    clip_fraction         | 0.127          |
|    clip_range            | 0.1            |
|    entropy_loss          | -1.73          |
|    explained_variance    | 0.888          |
|    icm_loss              | 3.01e-10       |
|    intrinsic_reward_mean | 3.0057834e-10  |
|    intrinsic_reward_std  | 1.12598306e-10 |
|    learning_rate         | 4.01e-05       |
|    loss                  | 0.0472         |
|    n_updates             | 820            |
|    policy_gradient_loss  | -0.0088        |
|    value_loss            | 0.199          |
---------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 195           |
|    ep_rew_mean           | 7             |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 207           |
|    time_elapsed          | 15969         |
|    total_timesteps       | 1695744       |
| train/                   |               |
|    approx_kl             | 0.0029248889  |
|    clip_fraction         | 0.144         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.7          |
|    explained_variance    | 0.898         |
|    icm_loss              | 2.99e-10      |
|    intrinsic_reward_mean | 2.9927175e-10 |
|    intrinsic_reward_std  | 1.3453953e-10 |
|    learning_rate         | 3.91e-05      |
|    loss                  | 0.0459        |
|    n_updates             | 824           |
|    policy_gradient_loss  | -0.00837      |
|    value_loss            | 0.209         |
--------------------------------------------
Eval num_timesteps=1700000, episode_reward=6.00 +/- 1.37
Episode length: 180.40 +/- 16.48
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 180           |
|    mean_reward           | 6             |
| time/                    |               |
|    total_timesteps       | 1700000       |
| train/                   |               |
|    approx_kl             | 0.002926449   |
|    clip_fraction         | 0.133         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.68         |
|    explained_variance    | 0.904         |
|    icm_loss              | 2.98e-10      |
|    intrinsic_reward_mean | 2.9842473e-10 |
|    intrinsic_reward_std  | 1.0676032e-10 |
|    learning_rate         | 3.8e-05       |
|    loss                  | 0.061         |
|    n_updates             | 828           |
|    policy_gradient_loss  | -0.0094       |
|    value_loss            | 0.187         |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 194      |
|    ep_rew_mean     | 6.97     |
| time/              |          |
|    fps             | 106      |
|    iterations      | 208      |
|    time_elapsed    | 16060    |
|    total_timesteps | 1703936  |
---------------------------------
---------------------------------------------
| rollout/                 |                |
|    ep_len_mean           | 193            |
|    ep_rew_mean           | 7.17           |
| time/                    |                |
|    fps                   | 106            |
|    iterations            | 209            |
|    time_elapsed          | 16132          |
|    total_timesteps       | 1712128        |
| train/                   |                |
|    approx_kl             | 0.0033317516   |
|    clip_fraction         | 0.149          |
|    clip_range            | 0.1            |
|    entropy_loss          | -1.69          |
|    explained_variance    | 0.92           |
|    icm_loss              | 2.99e-10       |
|    intrinsic_reward_mean | 2.9934544e-10  |
|    intrinsic_reward_std  | 1.08436135e-10 |
|    learning_rate         | 3.7e-05        |
|    loss                  | 0.0184         |
|    n_updates             | 832            |
|    policy_gradient_loss  | -0.00968       |
|    value_loss            | 0.163          |
---------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 196           |
|    ep_rew_mean           | 7.17          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 210           |
|    time_elapsed          | 16199         |
|    total_timesteps       | 1720320       |
| train/                   |               |
|    approx_kl             | 0.0028916155  |
|    clip_fraction         | 0.13          |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.67         |
|    explained_variance    | 0.899         |
|    icm_loss              | 2.97e-10      |
|    intrinsic_reward_mean | 2.974325e-10  |
|    intrinsic_reward_std  | 1.2566621e-10 |
|    learning_rate         | 3.6e-05       |
|    loss                  | 0.0397        |
|    n_updates             | 836           |
|    policy_gradient_loss  | -0.00924      |
|    value_loss            | 0.183         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 185           |
|    ep_rew_mean           | 7.09          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 211           |
|    time_elapsed          | 16277         |
|    total_timesteps       | 1728512       |
| train/                   |               |
|    approx_kl             | 0.0030720108  |
|    clip_fraction         | 0.135         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.7          |
|    explained_variance    | 0.898         |
|    icm_loss              | 3e-10         |
|    intrinsic_reward_mean | 2.9995942e-10 |
|    intrinsic_reward_std  | 1.1250625e-10 |
|    learning_rate         | 3.5e-05       |
|    loss                  | 0.0314        |
|    n_updates             | 840           |
|    policy_gradient_loss  | -0.00821      |
|    value_loss            | 0.194         |
--------------------------------------------
-------------------------------------------
| rollout/                 |              |
|    ep_len_mean           | 180          |
|    ep_rew_mean           | 7.17         |
| time/                    |              |
|    fps                   | 106          |
|    iterations            | 212          |
|    time_elapsed          | 16351        |
|    total_timesteps       | 1736704      |
| train/                   |              |
|    approx_kl             | 0.0029670915 |
|    clip_fraction         | 0.123        |
|    clip_range            | 0.1          |
|    entropy_loss          | -1.67        |
|    explained_variance    | 0.902        |
|    icm_loss              | 2.95e-10     |
|    intrinsic_reward_mean | 2.950341e-10 |
|    intrinsic_reward_std  | 1.255469e-10 |
|    learning_rate         | 3.39e-05     |
|    loss                  | 0.0499       |
|    n_updates             | 844          |
|    policy_gradient_loss  | -0.00934     |
|    value_loss            | 0.211        |
-------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 188           |
|    ep_rew_mean           | 7.39          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 213           |
|    time_elapsed          | 16426         |
|    total_timesteps       | 1744896       |
| train/                   |               |
|    approx_kl             | 0.0028781933  |
|    clip_fraction         | 0.133         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.63         |
|    explained_variance    | 0.895         |
|    icm_loss              | 2.98e-10      |
|    intrinsic_reward_mean | 2.9834818e-10 |
|    intrinsic_reward_std  | 1.1621962e-10 |
|    learning_rate         | 3.29e-05      |
|    loss                  | 0.0833        |
|    n_updates             | 848           |
|    policy_gradient_loss  | -0.0093       |
|    value_loss            | 0.211         |
--------------------------------------------
Eval num_timesteps=1750000, episode_reward=6.20 +/- 1.04
Episode length: 169.80 +/- 57.53
---------------------------------------------
| eval/                    |                |
|    mean_ep_length        | 170            |
|    mean_reward           | 6.2            |
| time/                    |                |
|    total_timesteps       | 1750000        |
| train/                   |                |
|    approx_kl             | 0.0028503109   |
|    clip_fraction         | 0.124          |
|    clip_range            | 0.1            |
|    entropy_loss          | -1.65          |
|    explained_variance    | 0.901          |
|    icm_loss              | 2.94e-10       |
|    intrinsic_reward_mean | 2.942984e-10   |
|    intrinsic_reward_std  | 1.11940505e-10 |
|    learning_rate         | 3.19e-05       |
|    loss                  | 0.0622         |
|    n_updates             | 852            |
|    policy_gradient_loss  | -0.00794       |
|    value_loss            | 0.195          |
---------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 7.25     |
| time/              |          |
|    fps             | 106      |
|    iterations      | 214      |
|    time_elapsed    | 16519    |
|    total_timesteps | 1753088  |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 183           |
|    ep_rew_mean           | 6.91          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 215           |
|    time_elapsed          | 16591         |
|    total_timesteps       | 1761280       |
| train/                   |               |
|    approx_kl             | 0.0026960587  |
|    clip_fraction         | 0.113         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.64         |
|    explained_variance    | 0.906         |
|    icm_loss              | 2.93e-10      |
|    intrinsic_reward_mean | 2.9314184e-10 |
|    intrinsic_reward_std  | 1.0390892e-10 |
|    learning_rate         | 3.09e-05      |
|    loss                  | 0.0673        |
|    n_updates             | 856           |
|    policy_gradient_loss  | -0.00821      |
|    value_loss            | 0.203         |
--------------------------------------------
---------------------------------------------
| rollout/                 |                |
|    ep_len_mean           | 176            |
|    ep_rew_mean           | 6.64           |
| time/                    |                |
|    fps                   | 106            |
|    iterations            | 216            |
|    time_elapsed          | 16673          |
|    total_timesteps       | 1769472        |
| train/                   |                |
|    approx_kl             | 0.002535304    |
|    clip_fraction         | 0.106          |
|    clip_range            | 0.1            |
|    entropy_loss          | -1.74          |
|    explained_variance    | 0.888          |
|    icm_loss              | 2.97e-10       |
|    intrinsic_reward_mean | 2.9720115e-10  |
|    intrinsic_reward_std  | 1.12808714e-10 |
|    learning_rate         | 2.98e-05       |
|    loss                  | 0.0635         |
|    n_updates             | 860            |
|    policy_gradient_loss  | -0.00716       |
|    value_loss            | 0.212          |
---------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 179           |
|    ep_rew_mean           | 6.98          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 217           |
|    time_elapsed          | 16745         |
|    total_timesteps       | 1777664       |
| train/                   |               |
|    approx_kl             | 0.0028875628  |
|    clip_fraction         | 0.115         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.64         |
|    explained_variance    | 0.906         |
|    icm_loss              | 2.94e-10      |
|    intrinsic_reward_mean | 2.9392144e-10 |
|    intrinsic_reward_std  | 1.2071537e-10 |
|    learning_rate         | 2.88e-05      |
|    loss                  | 0.0736        |
|    n_updates             | 864           |
|    policy_gradient_loss  | -0.00773      |
|    value_loss            | 0.199         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 178           |
|    ep_rew_mean           | 6.84          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 218           |
|    time_elapsed          | 16819         |
|    total_timesteps       | 1785856       |
| train/                   |               |
|    approx_kl             | 0.0028813337  |
|    clip_fraction         | 0.113         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.67         |
|    explained_variance    | 0.892         |
|    icm_loss              | 2.91e-10      |
|    intrinsic_reward_mean | 2.913349e-10  |
|    intrinsic_reward_std  | 1.1852402e-10 |
|    learning_rate         | 2.78e-05      |
|    loss                  | 0.0674        |
|    n_updates             | 868           |
|    policy_gradient_loss  | -0.00902      |
|    value_loss            | 0.21          |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 187           |
|    ep_rew_mean           | 7.13          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 219           |
|    time_elapsed          | 16892         |
|    total_timesteps       | 1794048       |
| train/                   |               |
|    approx_kl             | 0.002652108   |
|    clip_fraction         | 0.111         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.67         |
|    explained_variance    | 0.907         |
|    icm_loss              | 2.92e-10      |
|    intrinsic_reward_mean | 2.921518e-10  |
|    intrinsic_reward_std  | 1.3912635e-10 |
|    learning_rate         | 2.68e-05      |
|    loss                  | 0.0363        |
|    n_updates             | 872           |
|    policy_gradient_loss  | -0.00834      |
|    value_loss            | 0.197         |
--------------------------------------------
Eval num_timesteps=1800000, episode_reward=6.00 +/- 2.34
Episode length: 170.20 +/- 55.55
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 170           |
|    mean_reward           | 6             |
| time/                    |               |
|    total_timesteps       | 1800000       |
| train/                   |               |
|    approx_kl             | 0.0023501464  |
|    clip_fraction         | 0.103         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.66         |
|    explained_variance    | 0.905         |
|    icm_loss              | 2.92e-10      |
|    intrinsic_reward_mean | 2.9190697e-10 |
|    intrinsic_reward_std  | 1.0904905e-10 |
|    learning_rate         | 2.57e-05      |
|    loss                  | 0.0584        |
|    n_updates             | 876           |
|    policy_gradient_loss  | -0.00732      |
|    value_loss            | 0.195         |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 7.31     |
| time/              |          |
|    fps             | 106      |
|    iterations      | 220      |
|    time_elapsed    | 16980    |
|    total_timesteps | 1802240  |
---------------------------------
-------------------------------------------
| rollout/                 |              |
|    ep_len_mean           | 187          |
|    ep_rew_mean           | 7.21         |
| time/                    |              |
|    fps                   | 106          |
|    iterations            | 221          |
|    time_elapsed          | 17055        |
|    total_timesteps       | 1810432      |
| train/                   |              |
|    approx_kl             | 0.0024906248 |
|    clip_fraction         | 0.108        |
|    clip_range            | 0.1          |
|    entropy_loss          | -1.66        |
|    explained_variance    | 0.895        |
|    icm_loss              | 2.91e-10     |
|    intrinsic_reward_mean | 2.909411e-10 |
|    intrinsic_reward_std  | 1.056929e-10 |
|    learning_rate         | 2.47e-05     |
|    loss                  | 0.0796       |
|    n_updates             | 880          |
|    policy_gradient_loss  | -0.00665     |
|    value_loss            | 0.217        |
-------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 182           |
|    ep_rew_mean           | 6.96          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 222           |
|    time_elapsed          | 17124         |
|    total_timesteps       | 1818624       |
| train/                   |               |
|    approx_kl             | 0.00231351    |
|    clip_fraction         | 0.0927        |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.71         |
|    explained_variance    | 0.913         |
|    icm_loss              | 2.92e-10      |
|    intrinsic_reward_mean | 2.9177252e-10 |
|    intrinsic_reward_std  | 1.0239399e-10 |
|    learning_rate         | 2.37e-05      |
|    loss                  | 0.0522        |
|    n_updates             | 884           |
|    policy_gradient_loss  | -0.00785      |
|    value_loss            | 0.189         |
--------------------------------------------
---------------------------------------------
| rollout/                 |                |
|    ep_len_mean           | 182            |
|    ep_rew_mean           | 6.88           |
| time/                    |                |
|    fps                   | 106            |
|    iterations            | 223            |
|    time_elapsed          | 17202          |
|    total_timesteps       | 1826816        |
| train/                   |                |
|    approx_kl             | 0.0019350732   |
|    clip_fraction         | 0.0706         |
|    clip_range            | 0.1            |
|    entropy_loss          | -1.63          |
|    explained_variance    | 0.92           |
|    icm_loss              | 2.91e-10       |
|    intrinsic_reward_mean | 2.9086952e-10  |
|    intrinsic_reward_std  | 1.10707686e-10 |
|    learning_rate         | 2.27e-05       |
|    loss                  | 0.058          |
|    n_updates             | 888            |
|    policy_gradient_loss  | -0.00632       |
|    value_loss            | 0.184          |
---------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 184           |
|    ep_rew_mean           | 7.19          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 224           |
|    time_elapsed          | 17274         |
|    total_timesteps       | 1835008       |
| train/                   |               |
|    approx_kl             | 0.002060762   |
|    clip_fraction         | 0.0923        |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.61         |
|    explained_variance    | 0.892         |
|    icm_loss              | 2.88e-10      |
|    intrinsic_reward_mean | 2.87954e-10   |
|    intrinsic_reward_std  | 1.1245337e-10 |
|    learning_rate         | 2.16e-05      |
|    loss                  | 0.0542        |
|    n_updates             | 892           |
|    policy_gradient_loss  | -0.00629      |
|    value_loss            | 0.228         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 181           |
|    ep_rew_mean           | 7.51          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 225           |
|    time_elapsed          | 17351         |
|    total_timesteps       | 1843200       |
| train/                   |               |
|    approx_kl             | 0.002354748   |
|    clip_fraction         | 0.094         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.6          |
|    explained_variance    | 0.89          |
|    icm_loss              | 2.9e-10       |
|    intrinsic_reward_mean | 2.8963804e-10 |
|    intrinsic_reward_std  | 1.0562899e-10 |
|    learning_rate         | 2.06e-05      |
|    loss                  | 0.0675        |
|    n_updates             | 896           |
|    policy_gradient_loss  | -0.00726      |
|    value_loss            | 0.237         |
--------------------------------------------
Eval num_timesteps=1850000, episode_reward=6.40 +/- 1.42
Episode length: 192.70 +/- 23.02
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 193           |
|    mean_reward           | 6.4           |
| time/                    |               |
|    total_timesteps       | 1850000       |
| train/                   |               |
|    approx_kl             | 0.0019892724  |
|    clip_fraction         | 0.0784        |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.61         |
|    explained_variance    | 0.892         |
|    icm_loss              | 2.87e-10      |
|    intrinsic_reward_mean | 2.866519e-10  |
|    intrinsic_reward_std  | 1.0227812e-10 |
|    learning_rate         | 1.96e-05      |
|    loss                  | 0.0981        |
|    n_updates             | 900           |
|    policy_gradient_loss  | -0.00518      |
|    value_loss            | 0.235         |
--------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 7.61     |
| time/              |          |
|    fps             | 106      |
|    iterations      | 226      |
|    time_elapsed    | 17437    |
|    total_timesteps | 1851392  |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 188           |
|    ep_rew_mean           | 7.27          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 227           |
|    time_elapsed          | 17510         |
|    total_timesteps       | 1859584       |
| train/                   |               |
|    approx_kl             | 0.0020826003  |
|    clip_fraction         | 0.0768        |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.63         |
|    explained_variance    | 0.893         |
|    icm_loss              | 2.88e-10      |
|    intrinsic_reward_mean | 2.8801503e-10 |
|    intrinsic_reward_std  | 9.925836e-11  |
|    learning_rate         | 1.86e-05      |
|    loss                  | 0.063         |
|    n_updates             | 904           |
|    policy_gradient_loss  | -0.00655      |
|    value_loss            | 0.213         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 193           |
|    ep_rew_mean           | 7.39          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 228           |
|    time_elapsed          | 17581         |
|    total_timesteps       | 1867776       |
| train/                   |               |
|    approx_kl             | 0.0021351543  |
|    clip_fraction         | 0.0731        |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.6          |
|    explained_variance    | 0.902         |
|    icm_loss              | 2.87e-10      |
|    intrinsic_reward_mean | 2.8710884e-10 |
|    intrinsic_reward_std  | 1.0933537e-10 |
|    learning_rate         | 1.76e-05      |
|    loss                  | 0.0816        |
|    n_updates             | 908           |
|    policy_gradient_loss  | -0.00622      |
|    value_loss            | 0.224         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 198           |
|    ep_rew_mean           | 7.4           |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 229           |
|    time_elapsed          | 17654         |
|    total_timesteps       | 1875968       |
| train/                   |               |
|    approx_kl             | 0.0019477333  |
|    clip_fraction         | 0.0761        |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.62         |
|    explained_variance    | 0.9           |
|    icm_loss              | 2.86e-10      |
|    intrinsic_reward_mean | 2.8630687e-10 |
|    intrinsic_reward_std  | 1.0589957e-10 |
|    learning_rate         | 1.65e-05      |
|    loss                  | 0.0746        |
|    n_updates             | 912           |
|    policy_gradient_loss  | -0.00633      |
|    value_loss            | 0.211         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 183           |
|    ep_rew_mean           | 7.06          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 230           |
|    time_elapsed          | 17730         |
|    total_timesteps       | 1884160       |
| train/                   |               |
|    approx_kl             | 0.001867078   |
|    clip_fraction         | 0.0696        |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.63         |
|    explained_variance    | 0.93          |
|    icm_loss              | 2.86e-10      |
|    intrinsic_reward_mean | 2.8609728e-10 |
|    intrinsic_reward_std  | 1.0892783e-10 |
|    learning_rate         | 1.55e-05      |
|    loss                  | 0.0253        |
|    n_updates             | 916           |
|    policy_gradient_loss  | -0.00592      |
|    value_loss            | 0.154         |
--------------------------------------------
---------------------------------------------
| rollout/                 |                |
|    ep_len_mean           | 178            |
|    ep_rew_mean           | 7.26           |
| time/                    |                |
|    fps                   | 106            |
|    iterations            | 231            |
|    time_elapsed          | 17802          |
|    total_timesteps       | 1892352        |
| train/                   |                |
|    approx_kl             | 0.001954998    |
|    clip_fraction         | 0.0685         |
|    clip_range            | 0.1            |
|    entropy_loss          | -1.6           |
|    explained_variance    | 0.898          |
|    icm_loss              | 2.8e-10        |
|    intrinsic_reward_mean | 2.7980415e-10  |
|    intrinsic_reward_std  | 1.13585794e-10 |
|    learning_rate         | 1.45e-05       |
|    loss                  | 0.0903         |
|    n_updates             | 920            |
|    policy_gradient_loss  | -0.00625       |
|    value_loss            | 0.239          |
---------------------------------------------
Eval num_timesteps=1900000, episode_reward=6.60 +/- 1.02
Episode length: 204.20 +/- 56.93
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 204           |
|    mean_reward           | 6.6           |
| time/                    |               |
|    total_timesteps       | 1900000       |
| train/                   |               |
|    approx_kl             | 0.0016704592  |
|    clip_fraction         | 0.059         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.63         |
|    explained_variance    | 0.899         |
|    icm_loss              | 2.77e-10      |
|    intrinsic_reward_mean | 2.7688007e-10 |
|    intrinsic_reward_std  | 1.2409046e-10 |
|    learning_rate         | 1.35e-05      |
|    loss                  | 0.0808        |
|    n_updates             | 924           |
|    policy_gradient_loss  | -0.00672      |
|    value_loss            | 0.229         |
--------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 7.43     |
| time/              |          |
|    fps             | 106      |
|    iterations      | 232      |
|    time_elapsed    | 17895    |
|    total_timesteps | 1900544  |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 188           |
|    ep_rew_mean           | 7.34          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 233           |
|    time_elapsed          | 17967         |
|    total_timesteps       | 1908736       |
| train/                   |               |
|    approx_kl             | 0.0017700118  |
|    clip_fraction         | 0.0579        |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.61         |
|    explained_variance    | 0.9           |
|    icm_loss              | 2.75e-10      |
|    intrinsic_reward_mean | 2.7524522e-10 |
|    intrinsic_reward_std  | 1.1103423e-10 |
|    learning_rate         | 1.24e-05      |
|    loss                  | 0.0498        |
|    n_updates             | 928           |
|    policy_gradient_loss  | -0.00573      |
|    value_loss            | 0.226         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 186           |
|    ep_rew_mean           | 7.15          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 234           |
|    time_elapsed          | 18041         |
|    total_timesteps       | 1916928       |
| train/                   |               |
|    approx_kl             | 0.0015415145  |
|    clip_fraction         | 0.0402        |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.62         |
|    explained_variance    | 0.891         |
|    icm_loss              | 2.73e-10      |
|    intrinsic_reward_mean | 2.7272046e-10 |
|    intrinsic_reward_std  | 8.948108e-11  |
|    learning_rate         | 1.14e-05      |
|    loss                  | 0.0722        |
|    n_updates             | 932           |
|    policy_gradient_loss  | -0.00493      |
|    value_loss            | 0.264         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 187           |
|    ep_rew_mean           | 7.31          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 235           |
|    time_elapsed          | 18117         |
|    total_timesteps       | 1925120       |
| train/                   |               |
|    approx_kl             | 0.0015404444  |
|    clip_fraction         | 0.0506        |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.63         |
|    explained_variance    | 0.886         |
|    icm_loss              | 2.76e-10      |
|    intrinsic_reward_mean | 2.760306e-10  |
|    intrinsic_reward_std  | 1.2275603e-10 |
|    learning_rate         | 1.04e-05      |
|    loss                  | 0.089         |
|    n_updates             | 936           |
|    policy_gradient_loss  | -0.00568      |
|    value_loss            | 0.25          |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 187           |
|    ep_rew_mean           | 7.37          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 236           |
|    time_elapsed          | 18187         |
|    total_timesteps       | 1933312       |
| train/                   |               |
|    approx_kl             | 0.0010018903  |
|    clip_fraction         | 0.0266        |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.6          |
|    explained_variance    | 0.899         |
|    icm_loss              | 2.74e-10      |
|    intrinsic_reward_mean | 2.7383698e-10 |
|    intrinsic_reward_std  | 1.2422177e-10 |
|    learning_rate         | 9.36e-06      |
|    loss                  | 0.0992        |
|    n_updates             | 940           |
|    policy_gradient_loss  | -0.00467      |
|    value_loss            | 0.246         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 190           |
|    ep_rew_mean           | 7.48          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 237           |
|    time_elapsed          | 18260         |
|    total_timesteps       | 1941504       |
| train/                   |               |
|    approx_kl             | 0.0014163834  |
|    clip_fraction         | 0.0399        |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.61         |
|    explained_variance    | 0.9           |
|    icm_loss              | 2.71e-10      |
|    intrinsic_reward_mean | 2.7064445e-10 |
|    intrinsic_reward_std  | 1.2818568e-10 |
|    learning_rate         | 8.34e-06      |
|    loss                  | 0.0645        |
|    n_updates             | 944           |
|    policy_gradient_loss  | -0.00431      |
|    value_loss            | 0.23          |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 181           |
|    ep_rew_mean           | 6.99          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 238           |
|    time_elapsed          | 18337         |
|    total_timesteps       | 1949696       |
| train/                   |               |
|    approx_kl             | 0.0010993658  |
|    clip_fraction         | 0.024         |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.6          |
|    explained_variance    | 0.901         |
|    icm_loss              | 2.66e-10      |
|    intrinsic_reward_mean | 2.660545e-10  |
|    intrinsic_reward_std  | 1.1524039e-10 |
|    learning_rate         | 7.31e-06      |
|    loss                  | 0.0761        |
|    n_updates             | 948           |
|    policy_gradient_loss  | -0.00387      |
|    value_loss            | 0.242         |
--------------------------------------------
Eval num_timesteps=1950000, episode_reward=5.70 +/- 2.20
Episode length: 187.10 +/- 32.79
---------------------------------------------
| eval/                    |                |
|    mean_ep_length        | 187            |
|    mean_reward           | 5.7            |
| time/                    |                |
|    total_timesteps       | 1950000        |
| train/                   |                |
|    approx_kl             | 0.0012865297   |
|    clip_fraction         | 0.0345         |
|    clip_range            | 0.1            |
|    entropy_loss          | -1.6           |
|    explained_variance    | 0.862          |
|    icm_loss              | 2.68e-10       |
|    intrinsic_reward_mean | 2.6780528e-10  |
|    intrinsic_reward_std  | 1.13488934e-10 |
|    learning_rate         | 6.29e-06       |
|    loss                  | 0.117          |
|    n_updates             | 952            |
|    policy_gradient_loss  | -0.0031        |
|    value_loss            | 0.306          |
---------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 6.9      |
| time/              |          |
|    fps             | 106      |
|    iterations      | 239      |
|    time_elapsed    | 18427    |
|    total_timesteps | 1957888  |
---------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 182           |
|    ep_rew_mean           | 7.25          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 240           |
|    time_elapsed          | 18500         |
|    total_timesteps       | 1966080       |
| train/                   |               |
|    approx_kl             | 0.0009375161  |
|    clip_fraction         | 0.0153        |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.58         |
|    explained_variance    | 0.891         |
|    icm_loss              | 2.64e-10      |
|    intrinsic_reward_mean | 2.6416141e-10 |
|    intrinsic_reward_std  | 1.5392884e-10 |
|    learning_rate         | 5.26e-06      |
|    loss                  | 0.0992        |
|    n_updates             | 956           |
|    policy_gradient_loss  | -0.00263      |
|    value_loss            | 0.258         |
--------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 187           |
|    ep_rew_mean           | 7.38          |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 241           |
|    time_elapsed          | 18573         |
|    total_timesteps       | 1974272       |
| train/                   |               |
|    approx_kl             | 0.0009773215  |
|    clip_fraction         | 0.0242        |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.55         |
|    explained_variance    | 0.896         |
|    icm_loss              | 2.66e-10      |
|    intrinsic_reward_mean | 2.6603186e-10 |
|    intrinsic_reward_std  | 1.2191778e-10 |
|    learning_rate         | 4.24e-06      |
|    loss                  | 0.114         |
|    n_updates             | 960           |
|    policy_gradient_loss  | -0.00296      |
|    value_loss            | 0.252         |
--------------------------------------------
-------------------------------------------
| rollout/                 |              |
|    ep_len_mean           | 196          |
|    ep_rew_mean           | 7.71         |
| time/                    |              |
|    fps                   | 106          |
|    iterations            | 242          |
|    time_elapsed          | 18643        |
|    total_timesteps       | 1982464      |
| train/                   |              |
|    approx_kl             | 0.0007477631 |
|    clip_fraction         | 0.014        |
|    clip_range            | 0.1          |
|    entropy_loss          | -1.58        |
|    explained_variance    | 0.902        |
|    icm_loss              | 2.63e-10     |
|    intrinsic_reward_mean | 2.629167e-10 |
|    intrinsic_reward_std  | 1.397135e-10 |
|    learning_rate         | 3.22e-06     |
|    loss                  | 0.0866       |
|    n_updates             | 964          |
|    policy_gradient_loss  | -0.00239     |
|    value_loss            | 0.244        |
-------------------------------------------
---------------------------------------------
| rollout/                 |                |
|    ep_len_mean           | 199            |
|    ep_rew_mean           | 7.68           |
| time/                    |                |
|    fps                   | 106            |
|    iterations            | 243            |
|    time_elapsed          | 18714          |
|    total_timesteps       | 1990656        |
| train/                   |                |
|    approx_kl             | 0.00065193657  |
|    clip_fraction         | 0.00723        |
|    clip_range            | 0.1            |
|    entropy_loss          | -1.58          |
|    explained_variance    | 0.924          |
|    icm_loss              | 2.64e-10       |
|    intrinsic_reward_mean | 2.644639e-10   |
|    intrinsic_reward_std  | 1.10117616e-10 |
|    learning_rate         | 2.19e-06       |
|    loss                  | 0.0699         |
|    n_updates             | 968            |
|    policy_gradient_loss  | -0.00184       |
|    value_loss            | 0.189          |
---------------------------------------------
--------------------------------------------
| rollout/                 |               |
|    ep_len_mean           | 197           |
|    ep_rew_mean           | 7.6           |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 244           |
|    time_elapsed          | 18784         |
|    total_timesteps       | 1998848       |
| train/                   |               |
|    approx_kl             | 0.00039662136 |
|    clip_fraction         | 0.00226       |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.61         |
|    explained_variance    | 0.905         |
|    icm_loss              | 2.61e-10      |
|    intrinsic_reward_mean | 2.612684e-10  |
|    intrinsic_reward_std  | 1.2420524e-10 |
|    learning_rate         | 1.17e-06      |
|    loss                  | 0.0709        |
|    n_updates             | 972           |
|    policy_gradient_loss  | -0.00129      |
|    value_loss            | 0.234         |
--------------------------------------------
Eval num_timesteps=2000000, episode_reward=6.20 +/- 1.22
Episode length: 221.30 +/- 61.77
--------------------------------------------
| eval/                    |               |
|    mean_ep_length        | 221           |
|    mean_reward           | 6.2           |
| time/                    |               |
|    total_timesteps       | 2000000       |
| train/                   |               |
|    approx_kl             | 6.9875314e-06 |
|    clip_fraction         | 0             |
|    clip_range            | 0.1           |
|    entropy_loss          | -1.6          |
|    explained_variance    | 0.907         |
|    icm_loss              | 2.63e-10      |
|    intrinsic_reward_mean | 2.6302827e-10 |
|    intrinsic_reward_std  | 1.1383495e-10 |
|    learning_rate         | 1.44e-07      |
|    loss                  | 0.102         |
|    n_updates             | 976           |
|    policy_gradient_loss  | -0.000123     |
|    value_loss            | 0.238         |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 195      |
|    ep_rew_mean     | 7.61     |
| time/              |          |
|    fps             | 106      |
|    iterations      | 245      |
|    time_elapsed    | 18874    |
|    total_timesteps | 2007040  |
---------------------------------

Training complete. Final model saved to ./models/ppo_crafter_final.zip
WEEEE DONEEEE
Done at: Wed Oct 29 08:45:07 AM SAST 2025
